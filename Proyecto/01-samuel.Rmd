---
output:
  pdf_document: default
  html_document: default
---
```{r}
library(igraph)
library(curl)
library(tidyjson)
library(dplyr)
library(purrr)
library(tidyverse)
library(Matrix)
library(ggplot2)
```


# Analizando el *ssdeep*

Las funciones *hash* como *MD5*, *SHA256* o otras son útiles si queremos verificar la integridad de un archivo, su principio fundamental es que un pequeño cambio en el archivo (del orden de unos pocos bits) cambia la salida drásticamente.

En nuestro caso si queremos encontrar similitud entre *malware* no podemos usar esas funciones, porque si sabemos que un archivo es peligroso y tenemos su hash calculado, con cambiar un bit de ese archivo ya no lo podríamos detectar.

Por eso existe el programa *ssdeep*, que permite observar pequeñas diferencias entre archivos calculando el *CTPH* (parecido al *hash*).

Tenemos tres programas de ejemplo escritos en C, y queremos ver en qué porcentaje se parecen.

## sample1.c
```{c, eval=FALSE}
#include <stdio.h>

void main() {
  printf (“Hello World”);
}
```


## sample2.c
```{c, eval=FALSE}
#include <stdio.h>
int main(int argc, char *argv[]) {
for (int i = 0; i < 100; i++) {

        if (i%2 == 0) {
                i = i + 1;
        }
}
return 0;
}
```

## sample3.c
```{c, eval=FALSE}
#include <stdio.h>

void main() {
  int a = 5;
  printf (“Hello World: %d\n”, a);
}
```

Calculamos y comparamos el *ssdeep* de los tres programas:

```{bash, eval=FALSE}
$ ssdeep -s * > sample_ctph.ssd
$ ssdeep -m sample_ctph.ssd -s *
```

Obtenemos lo siguiente:

```{bash, eval=FALSE}
$ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample1 (100)
$ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample2 (63)
$ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample3 (80)
```

sample1.c es mucho más parecido a sample3.c (en un 80%, mientras que solo un 63% con sample2.c). 

## Analizando los json

De la misma manera, primero cargamos el directorio Android y el dataframe preprocesado, y luego calculamos el hash como con los ejemplos.

```{r, warning=FALSE, message=FALSE}
path <- "~/Documentos/LCC/ProyectoVT/Proyecto/Android2/"
nombres_ficheros <- list.files(path)
df <- read_csv("~/Documentos/LCC/ProyectoVT/Proyecto/virusTotal.csv")
```
Dentro del json, hay un par clave valor que almacena el *CTPH* calculado (*ssdeep*), así que para analizarlo, escribimos primero la siguiente función que crea un *dataframe* con todos los json:

```{r}
# Entrada: json
# Salida: dataframe 
get_ssdeep <- function(x) {
  json <- read_json(x)
  res <- json %>% gather_object() %>% filter(name == 'ssdeep') %>% as.data.frame()  
  return (res)
}

# Por cada archivo en la carpeta Android
# coge su ssdeep y los guarda en un dataframe
df_deep <- data.frame()
for (i in nombres_ficheros) {
  df_deep <- rbind(df_deep, get_ssdeep(paste0(path,i)))
}

# Limpia el dataframe
df_deep <- df_deep %>% select(..JSON)
colnames(df_deep) <- ('ssdeep')
```

Una vez tenemos los datos, comparamos dos a dos todos los *ssdeep* usando la función *adist* y almacenamos los índices y su distancia. Índices contiene la distancia del json *i* con el *j* para luego construir una matriz.

```{r, warning=FALSE, eval=FALSE}
# Coge los virus cuya distancia (parecido)
# en sus ssdeep sea menor que 2
# y los guarda en *indices*
indices <- c()
for (i in 1:nrow(df_deep)) {
  for (j in (i+1):nrow(df_deep)) {
    if ( adist(df_deep[i,], df_deep[j,]) == 1) {
      indices <- c(indices, i,j)
      }
  }
}
```

### Matriz de adyacencias

Creamos una matriz de adyacencias con todos los json, donde en este caso la posicion Mij = 1 si la distancia entre los hashes del archivo *i* y el *j* es igual a uno. 

__Cuanto menor es la distancia, menos diferencia hay entre los hashes y más código comparten.__

```{r, eval=FALSE}
  n <- nrow(df)
  Mat <- matrix(0, nrow = n, ncol = n)
  colnames(Mat) <- c(1:n)
  row.names(Mat) <- c(1:n)
  j <- 1
  while(j < length(indices)) {
    Mat[indices[j], indices[j+1]] <- 1
    j <- j+2
  }
```


Teniendo la matriz de adyacencias el siguiente paso es construir el grafo. Cada vértice es un json, y los arcos conectan json cuya distancia recibe por parámetro *get_adj_matrix*. Una vez se crea el grafo añadimos un atributo a las aristas con su distancia.

```{r}
# Entrada: distancia entre json
# Salida: Grafo=(V,E) 
#         V = json
#         E = distancias (ssdeep)
get_adj_matrix <- function(distancia) {
  
  indices <- c()
  distancias <- c()
  for (i in 1:nrow(df_deep)) {
    for (j in (i+1):nrow(df_deep)) {
      if ( adist(df_deep[i,], df_deep[j,]) <= distancia) {
        indices <- c(indices, i,j)
        distancias <- c(distancias, adist(df_deep[i,], df_deep[j,]))
      }
    }
  }
  
  n <- nrow(df)
  Mat <- matrix(0, nrow = n, ncol = n)
  colnames(Mat) <- c(1:n)
  row.names(Mat) <- c(1:n)
  j <- 1
  while(j < length(indices)) {
    Mat[indices[j], indices[j+1]] <- 1
    j <- j+2
  }

  G <- graph_from_adjacency_matrix(Mat, mode = 'undirected')
  G <- set_edge_attr(G, 'dist', value=distancias)
  
  return(G)
  
}
```


## Grafos

Dibujamos el siguiente grafo con aquellos json cuyas distancias son menores o iguales que uno. Esto quiere decir que, por ejemplo, el grupo 147, 153 y 46 comparte gran parte de código. 

```{r}
G <- get_adj_matrix(1)

Isolated = which(degree(G)==0)
G2 = delete.vertices(G, Isolated)

plot(G2, vertex.color='#ADD8E6', 
     edge.curved = .1, 
     vertex.size=20,
     edge.label=E(G2)$dist,
     vertex.frame.color = NA,
     layout=layout_nicely
     )
title("Distancias <= 1",cex.main=1,col.main="Black")
```

## Grafo completo, G1

Vamos a ver qué grafo se dibuja si restringimos menos la búsqueda y ponemos que saque todos los archivos que se parezcan como mínimo en un 60%.

```{r}
G1 <- get_adj_matrix(60)

plot(G1)
title("Grafo completo con distancia <= 60",cex.main=1,col.main="Black")

```


Se pueden ver varios grupos que forman componentes y muchos otros nodos aislados. Esto puede deberse a que los componentes corresponden a alguna variante del mismo malware.

Calculamos los componentes y guardamos los nodos de aquel más grande.

```{r}
c1 <- components(G1)
biggest1 <- which.max(c1$csize)
vids1 <- V(G1)[c1$membership==biggest1]

```

Dibujamos el subgrafo.

```{r}
plot(induced_subgraph(G1, vids1), edge.label=E(G1)$dist)
title("Mayor componente con pesos",cex.main=1,col.main="Black")

plot(induced_subgraph(G1, vids1), vertex.size=25)
title("Mayor componente sin pesos",cex.main=1,col.main="Black")

```

```{r, fig.asp=1.5}
plot(induced_subgraph(G1, vids1),vertex.size = 5, vertex.color = "#1e3f66", vertex.frame.color = 'red', vertex.label.cex = .7,  vertex.label = NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7)
title("Mayor componente",cex.main=1,col.main="Black")
```


### Análisis del componente

Usando el algoritmo pagerank y la función grado, ordenamos los nodos por importancia:

```{r}
subgrafo <- induced_subgraph(G1, vids1)
pg <- page.rank(subgrafo)

importancia <- data.frame(
  grado = degree(subgrafo),
  page_rank = pg$vector
)
importancia_sorted <- data.frame(
  grado = sort(degree(subgrafo), decreasing = TRUE),
  page_rank = sort(pg$vector, decreasing = TRUE)
)

knitr::kable(head(importancia_sorted, 10))

```

Resaltamos los nodos con grado mayor que cincuenta:

```{r, fig.asp=1.5}
plot(subgrafo, vertex.size=ifelse(importancia[V(subgrafo),][1]>50,5, 1), vertex.label=NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7)
```


Si vemos los tamaños de los componentes, hay uno con doce nodos:

```{r}
c1$csize
```

Lo dibujamos, en este caso los json comparten aproximadamente la mitad de código.

```{r}
vids2 <- V(G1)[c1$membership==12]
plot(induced_subgraph(G1, vids2), edge.label=E(G1)$dist, vertex.size=20)
```


### Algunos gráficos de G1

A continuación se muestran gráficas del componente grande de G1. Transformamos las fechas a formato *date* para poder dibujarlas.

```{r, warning=FALSE, message=FALSE, error=FALSE}
# Comparacion del componente grande de G1
v <- as(vids1, 'vector')
df_grafo <- subset(df, row.names(df) %in% v)

library(plotly)

df_grafo$scan_date <- as.POSIXct(df_grafo$scan_date, format="%Y-%m-%d %H:%M:%S")
df_grafo$first_seen <- as.POSIXct(df_grafo$first_seen, format="%Y-%m-%d %H:%M:%S")
colnames(df_grafo)[8] <- 'Pais'

```




Positivos a lo largo del tiempo (fecha de escáner).
```{r, warning=FALSE, message=FALSE}
p <- ggplot(df_grafo, aes(x=scan_date, y=positives)) +
  geom_line() + ylab('Positivos') + xlab('Fecha de escáner') + theme_bw()
ggplotly(p)
```



Tamaño de los json a lo largo del tiempo (fecha de escáner).
```{r, warning=FALSE, message=FALSE}
p2 <- ggplot(df_grafo, aes(x=scan_date, y=size)) +
  geom_line() + ylab('Tamaño') + xlab('Primera vez subido') + theme_bw()
ggplotly(p2)
```




Positivos a lo largo del tiempo (primera vez que se subió).
```{r, warning=FALSE, message=FALSE}
p3 <- ggplot(df_grafo, aes(x=first_seen, y=positives)) +
  geom_line() + ylab('Positivos') + xlab('Tiempo') + theme_bw()
ggplotly(p3)
```




Regiones desde donde se subió:
```{r, warning=FALSE, message=FALSE}
df_grafo %>% select(times_submitted, Pais) %>% group_by(Pais) %>% summarise(times_submitted=sum(times_submitted)) %>% 
ggplot(data=., aes(y=times_submitted)) + 
  geom_bar(mapping = aes(fill=Pais, x=Pais), stat = 'identity') + 
  theme_bw() 
```


Como los datos no están a escala, sumamos uno y aplicamos logaritmo para compararlos mejor:

```{r, warning=FALSE, message=FALSE}
p_regiones <- df_grafo %>% 
  select(times_submitted, Pais) %>% 
  group_by(Pais) %>% 
  summarise(times_submitted=sum(times_submitted)) %>% 
  ggplot(data=., aes(y=log(1+times_submitted))) + 
  geom_bar(mapping = aes(fill=Pais, x=Pais), stat = 'identity') + 
  ylab('Veces subido') + 
  xlab('País') +
  theme_bw() 

  
ggplotly(p_regiones)

```



Visualizamos el número de positivos por país:

```{r, warning=FALSE, message=FALSE}
p_positivos_pais <- df_grafo %>% 
  select(positives, Pais) %>% 
  group_by(Pais) %>% summarise(positives=sum(positives)) %>% 
  ggplot(data=., aes(y=positives)) + 
  geom_bar(mapping = aes(fill=Pais, x=Pais), stat = 'identity') + 
  ylab('Positivos') + 
  xlab('País') +
  theme_bw() 

ggplotly(p_positivos_pais)
```



## Grafo de distancias <= 10 para G

Calculamos y dibujamos las componentes del grafo con distancias menores o iguales que diez.
```{r}
G <- get_adj_matrix(10)

c <- components(G)
biggest <- which.max(c$csize)
vids <- V(G)[c$membership==biggest]
plot(induced_subgraph(G, vids), edge.label=E(G)$dist)
title("Distancias <= 10 con pesos",cex.main=1,col.main="Black")

plot(induced_subgraph(G, vids), vertex.size=25)
title("Distancias <= 10 sin pesos",cex.main=1,col.main="Black")

plot(induced_subgraph(G, vids),vertex.size = 10, vertex.color = "#1e3f66", vertex.frame.color = 'blue', vertex.label.cex = .7,  vertex.label = NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7)
title("Distancias <= 10",cex.main=1,col.main="Black")

```


## Análisis del mayor componente de G

Todos los archivos tienen el mismo tamaño, subidos desde California en menos de una hora. Con una media de 20  positivos, probablemente sean el mismo archivo.

```{r}
v <- as(vids, 'vector')

compare <- data.frame()
for (i in v) {
  compare <- rbind(compare, df[i,])
}

compare %>% select(size) %>% unique()
compare %>% select(submission.submitter_country) %>% unique()

times <- compare %>% select(first_seen, scan_date) %>% 
  mutate(first_seen = gsub('20[0-9]{2}-[0-9]+-[0-9]+', '', first_seen), scan_date = gsub('20[0-9]{2}-[0-9]+-[0-9]+', '', scan_date))
```

Primera y última vez que se subió:
```{r}
lapply(times[,1], max)
lapply(times[,1], min)
```


Media:

```{r}
# Media
compare %>% select(positives) %>% lapply(., mean)
```

Desviación típica:

```{r}
# Desviación típica
compare %>% select(positives) %>% lapply(., sd)
```


## Analizando los resultados de los antivirus de G

Vamos a ver qué resultado da cada antivirus a los json (vértices) del grafo G. Si dos antivirus dan el mismo resultado en archivos diferentes que sabemos que son casi iguales es probable que compartan motor.

La siguiente función recorre el directorio del dataset y crea un dataframe con los antivirus.
```{r, eval=FALSE}

# Entrada: Ruta al fichero
# Salida: Dataframe con resultados de los AV
get_results <- function(json) {
  
  json_data <- tidyjson::read_json(json)
  df_temp <- json_data %>% 
    gather_object() %>%   
    filter(name=='scans') %>% 
    spread_all() %>% 
    gather_object() %>% 
    select(ends_with('result')) %>% 
    .[1,] %>% 
    select(-last_col())
  
  
  return(df_temp) 
}


```



vids tiene los vértices de los grafos calculados. Pasamos los nombres a formato numérico con ceros a la izquierda.

```{r}
ficheros <- sapply(vids, function(x) paste0(sprintf("%04d", x), '.json') )
```

Creamos un dataframe y, por cada vértice, cogemos los escáneres.

```{r, eval=FALSE}
df_results <- data.frame()
for (i in ficheros) {
  df_results <- rbind.fill(df_results, get_results(paste0(path,i)))
}
write.csv(df_results, "~/Documentos/LCC/ProyectoVT/Proyecto/escaneres.csv")
```

Leemos y limpiamos el dataframe.

```{r, warning=FALSE, error=FALSE, message=FALSE}
df_results <- read_csv("~/Documentos/LCC/ProyectoVT/Proyecto/escaneres.csv")
#df_results <- df_results %>% select(-..JSON)
colnames(df_results) <- lapply(colnames(df_results), function(x) gsub('.result', '', x))

# Quitar columnas enteras NA
df_results <- df_results[, colSums(is.na(df_results)) != nrow(df_results)]

df_results <- df_results %>% select(-...1)
```

Cogemos la columna trece, que no tiene valores NA. Como en realidad todas las columnas son el mismo archivo con ver una sola nos sirve, y podemos ver cómo cada antivirus (excepto los que comparten motor) lo clasifican de manera distinta.

```{r}
j107 <- df_results[13,]
j107 <- t(j107)
knitr::kable(j107, col.names = c('0107.json'))
```



Si seleccionamos los AV Kasperky y ZoneAlarm se observa fácilmente que los resultados son idénticos y seguramente compartan el mismo motor.

```{r}
df_results %>% select(Kaspersky, ZoneAlarm) %>% head(., 10) %>% knitr::kable(.)
```


McAfee y McAfee GW Edition también, normal porque ambos son de McAfee
```{r}
df_results %>% select(McAfee, `McAfee-GW-Edition`) %>% head(., 10) %>% knitr::kable(.)
```





