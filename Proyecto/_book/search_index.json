[["index.html", "Virus Total Chapter 1 Preprocesado de datos 1.1 Inicio 1.2 Renombrando los archivos 1.3 Datos iniciales 1.4 Preprocesado", " Virus Total Samuel Sánchez Toca y Alejandro Medina Astorga 2022-06-08 Chapter 1 Preprocesado de datos Este es el proyecto realizado por Samuel Sánchez Toca y Alejandro Medina Astorga para la asignatura Laboratorio de Computación Científica para el dataset VirusTotal. 1.1 Inicio Primero tuvimos que preprocesar los datos que venían en formato json, por lo que tuvimos que extraerlos y decidir qué información iba a resultar más útil para su analisis. Para el preprocesado, hemos usado las librerías jsonlite, tidyjson, jsonlite y tidyverse para manejar los archivos. El dataset consta de casi doscientos ficheros con la información obtenida al analizarlos con distintos antivirus, el primer paso es, por tanto, ver cómo están estructurados los datos para decidir con qué información nos vamos a quedar. 1.2 Renombrando los archivos El siguiente script cambia los nombres a los archivos para facilitar su uso para hacer pruebas: a=1 for i in *.json; do new=$(printf &quot;%04d.json&quot; &quot;$a&quot;) mv -i -- &quot;$i&quot; &quot;$new&quot; let a=a+1 done 1.3 Datos iniciales Primero, listamos los nombres de todos los archivos y los guardamos en la variable nombres_ficheros: path &lt;- &quot;~/Documentos/LCC/ProyectoVT/Proyecto/Android2&quot; nombres_ficheros &lt;- list.files(path) Empezamos importando el primer fichero, usando la función read_json de la librería tidyjson obtenemos: json_0001 &lt;- tidyjson::read_json(paste0(path, &quot;/0001.json&quot;)) 1.4 Preprocesado Como indicamos antes, lo primero que hicimos fue leer el primer json para tener una idea de que es lo que tenemos que hacer. library(curl) library(tidyjson) library(dplyr) library(purrr) library(tidyverse) library(kableExtra) library(bookdown) json_data &lt;- tidyjson::read_json(&quot;~/Documentos/LCC/ProyectoVT/Proyecto/Android2/0001.json&quot;) tbl &lt;- json_data %&gt;% spread_all() tbl ## # A tbl_json: 1 x 337 tibble with a &quot;JSON&quot; attribute ## ..JSON document.id vhash scan_date first_seen total size authentihash times_submitted ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 &quot;{\\&quot;vhash\\&quot;:… 1 f6e2… 2021-11-… 2021-11-0… 61 1.96e6 NA 1 ## # … with 328 more variables: harmless_votes &lt;dbl&gt;, malicious_votes &lt;dbl&gt;, sha256 &lt;chr&gt;, ## # type &lt;chr&gt;, scan_id &lt;chr&gt;, unique_sources &lt;dbl&gt;, positives &lt;dbl&gt;, ssdeep &lt;chr&gt;, md5 &lt;chr&gt;, ## # permalink &lt;chr&gt;, sha1 &lt;chr&gt;, response_code &lt;dbl&gt;, community_reputation &lt;dbl&gt;, ## # verbose_msg &lt;chr&gt;, last_seen &lt;chr&gt;, additional_info.trid &lt;chr&gt;, ## # additional_info.magic &lt;chr&gt;, submission.submitter_region &lt;chr&gt;, submission.date &lt;chr&gt;, ## # submission.submitter_country &lt;chr&gt;, submission.filename &lt;chr&gt;, ## # additional_info.exiftool.MIMEType &lt;chr&gt;, … como vemos, spread_all transforma los pares clave valor en columnas de forma recursiva. Ahora vamos a hacer lo mismo pero con todos los ficheros: #path &lt;- &quot;~/GitHub/VirusTotal/ProyectoVT/Proyecto/Android2&quot; path &lt;- &quot;~/Documentos/LCC/ProyectoVT/Proyecto/Android2&quot; nombres_ficheros &lt;- list.files(path) j &lt;- 0 df &lt;- data.frame() for (i in nombres_ficheros) { filepath &lt;- file.path(path, paste0(i)) i &lt;- tidyjson::read_json(filepath) tbl &lt;- i %&gt;% spread_all(recursive = TRUE) nombre_l &lt;- tbl %&gt;% select(scan_date,first_seen,total,size,times_submitted,positives, submission.submitter_country,additional_info.exiftool.FileType) name &lt;- as.data.frame(nombre_l) name &lt;- name[-length(name)] df &lt;- rbind(df, name) j &lt;- j+1 } Con esto hemos preprocesado todos los archivos json y los metemos en un data frame. Hemos seleccionado las columnas scan_date, first_seen, total, size, times_submitted, country, fileType. Finalmente, mostramos el dataset resultante y lo guardamos como un .csv para tenerlo a mano. kable(df, booktabs = TRUE) %&gt;% kable_styling(font_size = 10) …1 …2 …3 scan_date first_seen total size times_submitted positives submission.submitter_country additional_info.exiftool.FileType 1 1 1 2021-11-03 08:53:57 2021-11-03 08:53:57 61 1956125 1 20 US ZIP 2 2 2 2021-11-03 00:26:03 2021-11-03 00:26:03 61 2667641 1 20 CA ZIP 3 3 3 2021-11-03 08:12:06 2021-11-03 08:12:06 61 3998656 1 18 UA ZIP 4 4 4 2021-11-03 08:44:36 2015-01-20 23:53:18 64 500276 263 26 IE ZIP 5 5 5 2021-11-03 08:54:32 2021-11-03 08:54:32 62 1956125 1 20 US ZIP 6 6 6 2021-11-03 08:22:50 2021-11-03 08:22:50 61 4137920 1 18 UA ZIP 7 7 7 2021-11-03 07:43:10 2021-06-03 18:29:36 61 3031864 2 15 UA ZIP 8 8 8 2021-11-03 00:25:19 2021-11-03 00:25:19 60 2669106 1 20 CA ZIP 9 9 9 2021-11-03 00:19:30 2021-11-03 00:19:30 60 2669106 1 21 CA ZIP 10 10 10 2021-11-03 08:00:30 2017-11-28 15:53:18 60 5884 4 25 CZ DEX 11 11 11 2021-11-03 07:53:39 2021-09-13 03:45:21 62 3035859 2 24 IT ZIP 12 12 12 2021-11-03 00:45:24 2021-11-03 00:45:24 61 2669106 1 19 CA ZIP 13 13 13 2021-11-03 08:44:15 2021-11-03 08:44:15 63 15202016 1 17 KR ZIP 14 14 14 2021-11-03 07:54:34 2021-11-03 07:54:34 63 8041344 1 17 SG ZIP 15 15 15 2021-11-03 09:16:16 2018-07-07 07:04:03 59 307908 11 20 CZ DEX 16 16 16 2021-11-03 08:16:40 2021-11-03 08:16:40 63 20353542 1 22 CA ZIP 17 17 17 2021-11-03 07:54:29 2021-10-18 08:50:50 62 6122860 2 18 IT ZIP 18 18 18 2021-11-03 00:00:26 2021-11-03 00:00:26 58 4804233 1 27 CA ZIP 19 19 19 2021-11-03 00:37:21 2021-11-03 00:37:21 60 1955983 1 19 US ZIP 20 20 20 2021-11-03 09:00:20 2019-10-01 03:13:27 56 48204 2 20 CZ DEX 21 21 21 2021-11-03 07:50:10 2016-12-03 11:18:21 53 366676 2 21 CZ DEX 22 22 22 2021-11-03 00:45:04 2021-11-03 00:45:04 62 2669106 1 22 CA ZIP 23 23 23 2021-11-03 07:40:40 2015-04-07 10:32:52 59 1674660 20 25 CZ DEX 24 24 24 2021-11-03 08:46:27 2017-10-27 21:12:27 59 424720 2 20 CZ DEX 25 25 25 2021-11-03 07:55:42 2013-09-11 08:40:27 63 394574 18 21 IT ZIP 26 26 26 2021-11-03 07:39:47 2018-04-16 22:03:40 59 5888 2 25 CZ DEX 27 27 27 2021-11-03 07:39:19 2019-09-01 04:43:45 60 7116 2 17 CZ DEX 28 28 28 2021-11-03 08:00:41 2017-08-18 16:45:45 59 17476 3 25 CZ DEX 29 29 29 2021-11-03 07:40:00 2021-01-16 23:49:02 63 7032582 255 19 MX ZIP 30 30 30 2021-11-03 07:39:44 2018-04-03 07:07:46 59 6004 3 26 CZ DEX 31 31 31 2021-11-03 00:10:40 2021-11-03 00:10:40 63 8801847 1 15 CA ZIP 32 32 32 2021-11-03 07:46:10 2017-12-28 05:51:06 59 774024 2 28 CZ DEX 33 33 33 2021-11-03 00:08:52 2021-11-03 00:08:52 56 4804234 1 26 CA ZIP 34 34 34 2021-11-03 08:16:18 2021-11-03 08:16:18 61 30711904 1 18 CA ZIP 35 35 35 2021-11-03 08:24:09 2021-08-03 12:09:52 62 4506129 2 29 LU ZIP 36 36 36 2021-11-03 07:50:25 2016-12-14 08:41:42 59 6160 4 22 CZ DEX 37 37 37 2021-11-03 08:28:49 2021-11-03 08:28:49 60 2947134 1 20 CA ZIP 38 38 38 2021-11-03 07:54:48 2021-10-18 02:40:12 62 6122893 2 21 IT ZIP 39 39 39 2021-11-03 08:32:38 2021-11-03 08:32:38 62 1956249 1 18 US ZIP 40 40 40 2021-11-03 08:18:57 2021-10-20 13:54:49 64 3903113 2 37 IT ZIP 41 41 41 2021-11-03 09:15:59 2021-11-03 09:15:59 62 543672 1 24 US ZIP 42 42 42 2021-11-03 08:16:20 2018-07-13 05:59:16 59 5884 2 25 CZ DEX 43 43 43 2021-11-03 07:54:09 2021-10-21 10:15:18 61 6126959 2 20 IT ZIP 44 44 44 2021-11-03 07:44:23 2016-01-06 01:51:40 59 1156 23 25 CZ DEX 45 45 45 2021-11-03 09:22:12 2015-08-12 17:10:54 62 860113 16 36 DE ZIP 46 46 46 2021-11-03 00:19:22 2021-11-03 00:19:22 60 2669106 1 19 CA ZIP 47 47 47 2021-11-03 08:15:31 2021-11-03 08:15:31 62 1955964 1 21 MX ZIP 48 48 48 2021-11-03 00:46:00 2021-11-03 00:46:00 61 2669106 1 20 CA ZIP 49 49 49 2021-11-03 09:14:16 2021-11-03 09:14:16 60 15721098 1 16 KR ZIP 50 50 50 2021-11-03 07:33:53 2019-09-08 03:35:40 62 63071192 7 15 GB ZIP 51 51 51 2021-11-03 00:01:05 2021-11-03 00:01:05 63 4804236 1 27 CA ZIP 52 52 52 2021-11-03 08:21:38 2021-11-03 08:21:38 61 601452 1 25 US ZIP 53 53 53 2021-11-03 07:55:52 2021-09-13 06:50:57 59 3035852 2 21 IT ZIP 54 54 54 2021-11-03 09:13:09 2021-11-03 09:13:09 61 1956174 1 20 BD ZIP 55 55 55 2021-11-03 00:28:18 2021-11-03 00:28:18 63 14898946 1 15 KR ZIP 56 56 56 2021-11-03 08:45:37 2021-11-03 08:45:37 62 10186 1 30 IN ZIP 57 57 57 2021-11-03 08:12:18 2017-03-05 15:29:33 59 1156 5 25 CZ DEX 58 58 58 2021-11-03 08:03:59 2021-11-03 08:03:59 63 15410862 1 18 KR ZIP 59 59 59 2021-11-03 08:34:18 2019-01-27 06:15:44 60 153424 2 26 CZ DEX 60 60 60 2021-11-03 07:55:19 2021-10-22 23:35:21 64 3394334 5 26 IT ZIP 61 61 61 2021-11-03 07:39:52 2015-05-09 02:35:41 59 214120 4 22 CZ DEX 62 62 62 2021-11-03 07:50:15 2016-12-24 15:14:13 59 17984 2 23 CZ DEX 63 63 63 2021-11-03 07:38:33 2019-05-13 04:00:23 59 7108008 3 16 CZ DEX 64 64 64 2021-11-03 07:37:40 2021-11-03 07:37:40 60 107759215 1 17 US ZIP 65 65 65 2021-11-03 08:02:16 2015-02-06 17:44:14 61 183459 11 31 DE ZIP 66 66 66 2021-11-03 00:28:10 2021-11-03 00:28:10 60 2669106 1 19 CA ZIP 67 67 67 2021-11-03 08:33:19 2021-11-03 08:33:19 62 1956249 1 17 US ZIP 68 68 68 2021-11-03 08:21:56 2021-11-03 08:21:56 63 3758923 1 20 CA ZIP 69 69 69 2021-11-03 08:46:24 2021-11-03 08:46:24 59 1998932 1 16 CZ DEX 70 70 70 2021-11-03 07:55:40 2021-10-19 04:55:25 62 6122862 2 23 IT ZIP 71 71 71 2021-11-03 07:56:08 2017-11-18 01:30:57 59 5904 4 23 CZ DEX 72 72 72 2021-11-03 07:38:41 2021-10-23 15:26:09 59 2111216 2 16 CZ DEX 73 73 73 2021-11-03 07:40:14 2015-06-04 08:59:13 58 621512 3 26 CZ DEX 74 74 74 2021-11-03 00:27:26 2021-11-03 00:27:26 59 2669106 1 17 CA ZIP 75 75 75 2021-11-03 09:21:06 2015-05-26 01:53:26 63 14498070 6 21 DE ZIP 76 76 76 2021-11-03 07:40:52 2016-08-15 05:53:10 59 7612932 2 17 CZ DEX 77 77 77 2021-11-03 00:19:25 2021-11-03 00:19:25 62 2669106 1 20 CA ZIP 78 78 78 2021-11-03 08:06:25 2021-11-03 08:06:25 62 699499 1 17 IE ZIP 79 79 79 2021-11-03 07:51:57 2016-10-28 05:09:58 58 4531604 20 15 CZ DEX 80 80 80 2021-11-03 08:12:09 2020-02-05 08:27:22 60 586168 33 27 CZ DEX 81 81 81 2021-11-03 08:06:24 2021-11-03 08:06:24 63 8728803 1 16 SG ZIP 82 82 82 2021-11-03 08:46:53 2021-11-03 08:46:53 62 2249530 1 16 CA ZIP 83 83 83 2021-11-03 07:44:14 2017-05-25 14:27:13 58 730228 2 15 CZ DEX 84 84 84 2021-11-03 00:25:46 2021-11-03 00:25:46 61 2669106 1 21 CA ZIP 85 85 85 2021-11-03 08:30:37 2021-11-03 08:30:37 61 178355426 1 15 RU ZIP 86 86 86 2021-11-03 08:40:13 2021-11-03 08:40:13 59 20316 1 19 CZ DEX 87 87 87 2021-11-03 07:50:12 2016-06-15 18:39:32 60 332868 3 23 CZ DEX 88 88 88 2021-11-03 07:46:05 2021-11-03 07:46:05 62 773772 1 35 US ZIP 89 89 89 2021-11-03 07:40:04 2018-04-06 06:35:31 59 6012 2 27 CZ DEX 90 90 90 2021-11-03 09:13:36 2021-11-03 09:13:36 63 14941837 1 17 KR ZIP 91 91 91 2021-11-03 07:52:13 2016-01-30 10:19:53 59 1152 29 26 CZ DEX 92 92 92 2021-11-03 07:40:00 2017-10-27 12:02:22 59 1914076 5 22 CZ DEX 93 93 93 2021-11-03 07:39:27 2019-01-27 15:37:29 60 560344 2 25 CZ DEX 94 94 94 2021-11-03 07:40:11 2018-08-07 23:47:10 60 5884 2 27 CZ DEX 95 95 95 2021-11-03 00:17:42 2021-11-03 00:17:42 63 4804230 1 28 CA ZIP 96 96 96 2021-11-03 07:40:21 2018-01-03 15:40:44 59 6000 4 27 CZ DEX 97 97 97 2021-11-03 08:44:11 2015-06-01 05:49:59 58 674204 3 15 CZ DEX 98 98 98 2021-11-03 07:49:58 2021-11-03 07:49:58 60 2678380 1 20 US ZIP 99 99 99 2021-11-03 08:27:19 2021-11-03 08:27:19 59 2228618 1 16 CA ZIP 100 100 100 2021-11-03 00:26:19 2021-11-03 00:26:19 62 2669106 1 21 CA ZIP 101 101 101 2021-11-03 09:14:01 2021-11-03 09:14:01 60 356169 1 21 IN ZIP 102 102 102 2021-11-03 07:40:25 2018-01-09 04:16:36 60 5996 3 24 CZ DEX 103 103 103 2021-11-03 09:06:44 2018-06-30 02:26:29 63 53604603 3 16 FR ZIP 104 104 104 2021-11-03 09:24:18 2017-03-14 20:20:29 58 787644 2 16 CZ DEX 105 105 105 2021-11-03 08:17:09 2021-11-03 08:17:09 63 21035680 1 23 CA ZIP 106 106 106 2021-11-03 00:28:16 2021-11-03 00:28:16 53 2669106 1 17 CA ZIP 107 107 107 2021-11-03 00:26:07 2021-11-03 00:26:07 61 2669106 1 23 CA ZIP 108 108 108 2021-11-03 09:25:14 2021-11-03 09:25:14 62 14898948 1 17 IE ZIP 109 109 109 2021-11-03 08:00:04 2021-11-03 08:00:04 62 1975464 1 19 US ZIP 110 110 110 2021-11-03 07:52:59 2021-10-20 00:56:21 60 6122851 4 19 IT ZIP 111 111 111 2021-11-03 08:58:14 2019-11-29 00:24:50 59 4385200 29 21 CZ DEX 112 112 112 2021-11-03 08:22:22 2021-11-03 08:22:22 59 3916196 1 15 CZ DEX 113 113 113 2021-11-03 00:37:49 2021-11-03 00:37:49 59 2669106 1 20 CA ZIP 114 114 114 2021-11-03 08:38:43 2021-11-03 08:38:43 61 2249617 1 16 CA ZIP 115 115 115 2021-11-03 08:31:47 2021-11-03 08:31:47 62 2111871 1 23 US ZIP 116 116 116 2021-11-03 08:06:13 2018-01-18 11:53:01 59 5988 3 24 CZ DEX 117 117 117 2021-11-03 08:28:35 2018-05-18 01:46:26 60 5892 2 25 CZ DEX 118 118 118 2021-11-03 08:06:25 2021-11-03 08:06:25 59 586008 1 17 CZ DEX 119 119 119 2021-11-03 08:04:19 2021-11-03 08:04:19 63 14898951 1 17 KR ZIP 120 120 120 2021-11-03 09:00:13 2017-08-06 05:32:54 59 659384 2 23 CZ DEX 121 121 121 2021-11-03 00:27:16 2021-11-03 00:27:16 62 2669106 1 17 CA ZIP 122 122 122 2021-11-03 08:06:09 2017-11-20 04:30:10 59 6008 5 28 CZ DEX 123 123 123 2021-11-03 07:56:14 2017-06-16 08:36:48 59 6040 2 21 CZ DEX 124 124 124 2021-11-03 08:28:15 2021-11-03 08:28:15 62 1719960 1 17 CA ZIP 125 125 125 2021-11-03 07:53:30 2021-10-22 12:02:25 63 3390238 2 26 IT ZIP 126 126 126 2021-11-03 00:19:33 2021-11-03 00:19:33 56 2669106 1 17 CA ZIP 127 127 127 2021-11-03 08:27:16 2021-11-03 08:27:16 62 2667888 1 20 CA ZIP 128 128 128 2021-11-03 00:27:35 2021-11-03 00:27:35 61 2669106 1 21 CA ZIP 129 129 129 2021-11-03 08:20:52 2021-11-03 08:20:52 60 2667641 1 22 CA ZIP 130 130 130 2021-11-03 00:35:56 2021-11-03 00:35:56 62 2669106 1 21 CA ZIP 131 131 131 2021-11-03 07:53:47 2021-11-03 07:53:47 63 14909972 1 16 KR ZIP 132 132 132 2021-11-03 08:06:27 2021-10-07 05:55:23 59 2150408 3 17 CZ DEX 133 133 133 2021-11-03 08:53:57 2021-11-03 08:53:57 62 1956125 1 20 US ZIP 134 134 134 2021-11-03 00:31:34 2021-11-03 00:31:34 60 2669106 1 21 CA ZIP 135 135 135 2021-11-03 07:53:50 2021-09-12 02:34:46 62 3357189 2 23 IT ZIP 136 136 136 2021-11-03 08:39:03 2021-11-03 08:39:03 64 3421363 1 28 CA ZIP 137 137 137 2021-11-03 07:52:22 2019-07-24 03:58:46 58 10898768 2 17 CZ DEX 138 138 138 2021-11-03 07:52:39 2021-10-24 12:31:54 64 3394319 2 26 IT ZIP 139 139 139 2021-11-03 00:31:03 2021-11-03 00:31:03 58 2669106 1 18 CA ZIP 140 140 140 2021-11-03 07:40:18 2019-01-25 08:16:16 59 1913112 4 20 CZ DEX 141 141 141 2021-11-03 08:37:57 2021-11-03 08:37:57 62 2667641 1 21 CA ZIP 142 142 142 2021-11-03 08:12:27 2018-03-01 18:26:47 60 5996 5 28 CZ DEX 143 143 143 2021-11-03 08:00:37 2017-12-17 11:36:32 52 5900 3 20 CZ DEX 144 144 144 2021-11-03 09:01:19 2021-11-03 09:01:19 57 4804230 1 23 CA ZIP 145 145 145 2021-11-03 08:43:29 2021-11-03 08:43:29 62 1956297 1 20 US ZIP 146 146 146 2021-11-03 08:34:35 2018-11-12 14:06:22 60 5880 2 26 CZ DEX 147 147 147 2021-11-03 00:25:15 2021-11-03 00:25:15 60 2669106 1 21 CA ZIP 148 148 148 2021-11-03 00:19:41 2021-11-03 00:19:41 62 2669106 1 19 CA ZIP 149 149 149 2021-11-03 07:40:07 2017-11-26 01:51:32 59 6000 6 25 CZ DEX 150 150 150 2021-11-03 09:00:52 2021-11-03 09:00:52 62 4804234 1 28 CA ZIP 151 151 151 2021-11-03 08:22:25 2017-07-17 13:07:00 59 5888 3 26 CZ DEX 152 152 152 2021-11-03 09:12:48 2021-10-20 08:44:12 57 7225742 2 21 US ZIP 153 153 153 2021-11-03 00:21:08 2021-11-03 00:21:08 61 2669106 1 20 CA ZIP 154 154 154 2021-11-03 08:23:29 2021-11-03 08:23:29 61 1956101 1 18 IN ZIP 155 155 155 2021-11-03 00:04:45 2014-02-23 02:22:08 58 34424 4 19 CZ DEX 156 156 156 2021-11-03 07:56:23 2021-10-19 13:37:27 62 6122901 2 26 IT ZIP 157 157 157 2021-11-03 00:25:14 2021-11-03 00:25:14 60 2669106 1 20 CA ZIP 158 158 158 2021-11-03 07:56:18 2021-11-03 07:56:18 62 8043761 1 19 SG ZIP 159 159 159 2021-11-03 00:26:11 2021-11-03 00:26:11 61 2669106 1 22 CA ZIP 160 160 160 2021-11-03 08:41:30 2021-11-03 08:41:30 62 824232 1 36 FR ZIP 161 161 161 2021-11-03 07:52:28 2021-10-19 13:53:09 62 6122902 2 27 IT ZIP 162 162 162 2021-11-03 08:21:46 2019-06-17 11:13:57 62 1928652 3543 16 BO ZIP 163 163 163 2021-11-03 09:01:28 2021-11-03 09:01:28 61 1955964 1 20 PH ZIP 164 164 164 2021-11-03 07:56:03 2021-10-24 07:14:49 63 3394284 5 25 IT ZIP 165 165 165 2021-11-03 08:00:54 2021-11-03 08:00:54 62 1975464 1 19 US ZIP 166 166 166 2021-11-03 09:20:31 2021-11-03 09:20:31 63 4804234 1 31 CA ZIP 167 167 167 2021-11-03 09:18:46 2021-11-03 09:18:46 61 4804234 1 28 CA ZIP 168 168 168 2021-11-03 08:17:00 2021-11-03 08:17:00 61 1467913 1 22 US ZIP 169 169 169 2021-11-03 00:20:03 2021-11-03 00:20:03 61 2669106 1 20 CA ZIP 170 170 170 2021-11-03 07:56:11 2012-12-18 03:26:17 59 6392 6 28 CZ DEX 171 171 171 2021-11-03 08:58:50 2021-11-03 08:58:50 61 4804233 1 29 CA ZIP 172 172 172 2021-11-03 08:30:36 2021-11-03 08:30:36 62 2667888 1 22 CA ZIP 173 173 173 2021-11-03 00:36:04 2021-11-03 00:36:04 60 2669106 1 20 CA ZIP 174 174 174 2021-11-03 07:39:56 2018-09-05 12:42:53 60 2795296 2 25 CZ DEX 175 175 175 2021-11-03 08:23:25 2021-11-03 08:23:25 63 15645802 1 16 NA ZIP 176 176 176 2021-11-03 09:00:45 2021-11-03 09:00:45 60 4804237 1 27 CA ZIP 177 177 177 2021-11-03 07:50:05 2021-11-03 07:50:05 62 1956078 1 20 US ZIP 178 178 178 2021-11-03 07:46:12 2015-04-15 10:15:15 59 11212 18 21 CZ DEX 179 179 179 2021-11-03 09:20:54 2021-11-03 09:20:54 63 4804234 1 29 CA ZIP 180 180 180 2021-11-01 15:38:30 2021-11-01 15:38:30 62 1513084 1 17 US ZIP 181 181 181 2021-11-03 08:30:58 2017-03-20 21:27:22 62 1629650 3 15 LU ZIP 182 182 182 2021-11-03 09:20:46 2021-11-03 09:20:46 62 4804234 1 29 CA ZIP 183 183 183 2021-11-03 08:46:17 2021-11-03 08:46:17 62 2369760 1 19 CA ZIP #write.csv(df,&quot;~/GitHub/VirusTotal/ProyectoVT/Proyecto/virusTotal.csv&quot;, row.names = FALSE)## Importar dataset write.csv(df, &quot;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotal.csv&quot;) write.csv(df, &quot;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotalCap2.csv&quot;) "],["analisis-exploratorio.html", "Chapter 2 Analisis exploratorio 2.1 Dplyr 2.2 Visualizacion", " Chapter 2 Analisis exploratorio 2.1 Dplyr Primero cargamos las librerias y leemos el csv que anteriormente generamos library(dplyr) library(tidyverse) library(kableExtra) #virusTotal &lt;- read.csv(&#39;~/GitHub/VirusTotal/ProyectoVT/Proyecto/virusTotal.csv&#39;) virusTotal &lt;- read.csv(&#39;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotal.csv&#39;) Vamos a añadir una columna year y cambiamos los nombres de algunas columnas para que sean más legibles. virusTotal &lt;- virusTotal %&gt;% mutate(Year = substr(virusTotal$first_seen, 0, 4)) virusTotal &lt;- virusTotal %&gt;% rename(country = submission.submitter_country) virusTotal &lt;- virusTotal %&gt;% rename(file_type = additional_info.exiftool.FileType) kable(head(virusTotal,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) X …1 …2 …3 scan_date first_seen total size times_submitted positives country file_type Year 1 1 1 1 2021-11-03 08:53:57 2021-11-03 08:53:57 61 1956125 1 20 US ZIP 2021 2 2 2 2 2021-11-03 00:26:03 2021-11-03 00:26:03 61 2667641 1 20 CA ZIP 2021 3 3 3 3 2021-11-03 08:12:06 2021-11-03 08:12:06 61 3998656 1 18 UA ZIP 2021 4 4 4 4 2021-11-03 08:44:36 2015-01-20 23:53:18 64 500276 263 26 IE ZIP 2015 5 5 5 5 2021-11-03 08:54:32 2021-11-03 08:54:32 62 1956125 1 20 US ZIP 2021 6 6 6 6 2021-11-03 08:22:50 2021-11-03 08:22:50 61 4137920 1 18 UA ZIP 2021 7 7 7 7 2021-11-03 07:43:10 2021-06-03 18:29:36 61 3031864 2 15 UA ZIP 2021 8 8 8 8 2021-11-03 00:25:19 2021-11-03 00:25:19 60 2669106 1 20 CA ZIP 2021 9 9 9 9 2021-11-03 00:19:30 2021-11-03 00:19:30 60 2669106 1 21 CA ZIP 2021 10 10 10 10 2021-11-03 08:00:30 2017-11-28 15:53:18 60 5884 4 25 CZ DEX 2017 Vamos a encontrar los 10 archivos más pesados del dataset virusTotal1 &lt;- virusTotal %&gt;% arrange(desc(size)) %&gt;% slice(1:10) kable(head(virusTotal1,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) X …1 …2 …3 scan_date first_seen total size times_submitted positives country file_type Year 85 85 85 85 2021-11-03 08:30:37 2021-11-03 08:30:37 61 178355426 1 15 RU ZIP 2021 64 64 64 64 2021-11-03 07:37:40 2021-11-03 07:37:40 60 107759215 1 17 US ZIP 2021 50 50 50 50 2021-11-03 07:33:53 2019-09-08 03:35:40 62 63071192 7 15 GB ZIP 2019 103 103 103 103 2021-11-03 09:06:44 2018-06-30 02:26:29 63 53604603 3 16 FR ZIP 2018 34 34 34 34 2021-11-03 08:16:18 2021-11-03 08:16:18 61 30711904 1 18 CA ZIP 2021 105 105 105 105 2021-11-03 08:17:09 2021-11-03 08:17:09 63 21035680 1 23 CA ZIP 2021 16 16 16 16 2021-11-03 08:16:40 2021-11-03 08:16:40 63 20353542 1 22 CA ZIP 2021 49 49 49 49 2021-11-03 09:14:16 2021-11-03 09:14:16 60 15721098 1 16 KR ZIP 2021 175 175 175 175 2021-11-03 08:23:25 2021-11-03 08:23:25 63 15645802 1 16 NA ZIP 2021 58 58 58 58 2021-11-03 08:03:59 2021-11-03 08:03:59 63 15410862 1 18 KR ZIP 2021 Pasamos a ordenar el dataset dependiendo del tipo del archivo virusTotal2 &lt;- virusTotal %&gt;% arrange(file_type) kable(head(virusTotal2,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) X …1 …2 …3 scan_date first_seen total size times_submitted positives country file_type Year 10 10 10 10 2021-11-03 08:00:30 2017-11-28 15:53:18 60 5884 4 25 CZ DEX 2017 15 15 15 15 2021-11-03 09:16:16 2018-07-07 07:04:03 59 307908 11 20 CZ DEX 2018 20 20 20 20 2021-11-03 09:00:20 2019-10-01 03:13:27 56 48204 2 20 CZ DEX 2019 21 21 21 21 2021-11-03 07:50:10 2016-12-03 11:18:21 53 366676 2 21 CZ DEX 2016 23 23 23 23 2021-11-03 07:40:40 2015-04-07 10:32:52 59 1674660 20 25 CZ DEX 2015 24 24 24 24 2021-11-03 08:46:27 2017-10-27 21:12:27 59 424720 2 20 CZ DEX 2017 26 26 26 26 2021-11-03 07:39:47 2018-04-16 22:03:40 59 5888 2 25 CZ DEX 2018 27 27 27 27 2021-11-03 07:39:19 2019-09-01 04:43:45 60 7116 2 17 CZ DEX 2019 28 28 28 28 2021-11-03 08:00:41 2017-08-18 16:45:45 59 17476 3 25 CZ DEX 2017 30 30 30 30 2021-11-03 07:39:44 2018-04-03 07:07:46 59 6004 3 26 CZ DEX 2018 Mostrar los archivos que son más pesados que la media mediaSize &lt;- mean(virusTotal$size) mediaSize ## [1] 5621897 virusTotal3 &lt;- virusTotal %&gt;% filter(virusTotal$size &gt; mediaSize)%&gt;% arrange(size) kable(head(virusTotal3,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) X …1 …2 …3 scan_date first_seen total size times_submitted positives country file_type Year 110 110 110 110 2021-11-03 07:52:59 2021-10-20 00:56:21 60 6122851 4 19 IT ZIP 2021 17 17 17 17 2021-11-03 07:54:29 2021-10-18 08:50:50 62 6122860 2 18 IT ZIP 2021 70 70 70 70 2021-11-03 07:55:40 2021-10-19 04:55:25 62 6122862 2 23 IT ZIP 2021 38 38 38 38 2021-11-03 07:54:48 2021-10-18 02:40:12 62 6122893 2 21 IT ZIP 2021 156 156 156 156 2021-11-03 07:56:23 2021-10-19 13:37:27 62 6122901 2 26 IT ZIP 2021 161 161 161 161 2021-11-03 07:52:28 2021-10-19 13:53:09 62 6122902 2 27 IT ZIP 2021 43 43 43 43 2021-11-03 07:54:09 2021-10-21 10:15:18 61 6126959 2 20 IT ZIP 2021 29 29 29 29 2021-11-03 07:40:00 2021-01-16 23:49:02 63 7032582 255 19 MX ZIP 2021 63 63 63 63 2021-11-03 07:38:33 2019-05-13 04:00:23 59 7108008 3 16 CZ DEX 2019 152 152 152 152 2021-11-03 09:12:48 2021-10-20 08:44:12 57 7225742 2 21 US ZIP 2021 Buscamos el primer archivo registrado virusTotal4 &lt;- virusTotal %&gt;% arrange(first_seen)%&gt;% select(first_seen,Year,size,times_submitted,positives,file_type)%&gt;% slice(1) kable(head(virusTotal4,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) first_seen Year size times_submitted positives file_type 2012-12-18 03:26:17 2012 6392 6 28 DEX Por último vamos a contar los archivos dependiendo de su tipo virusTotal5 &lt;- virusTotal%&gt;% group_by(file_type)%&gt;% summarise( n = n() ) kable(head(virusTotal5,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) file_type n DEX 58 ZIP 125 2.2 Visualizacion Vamos a analizar el dataset a traves de los gráficos. Empezamos con el numero de positivos y las veces que se repiten con Histogramas. library(ggplot2) ggplot(virusTotal, aes(x=positives)) + geom_histogram(binwidth = 0.5) + xlab(&#39;Positivos&#39;) + ylab(&#39;Nº de veces&#39;) + theme_bw() Aquí vemos una gráfica de los años según las veces que aparecen en el dataset ggplot(virusTotal, aes(x=Year)) + geom_bar(binwidth = 0.9) + xlab(&#39;Año&#39;) + ylab(&#39;Nº de veces&#39;) + theme_bw() ## Warning: Ignoring unknown parameters: binwidth Ahora visualizaremos lo anterior pero con los colores dependiendo del país ggplot(virusTotal, aes(x=positives, fill=country)) + geom_histogram(binwidth = 0.5) + xlab(&#39;Año&#39;) + ylab(&#39;Nº de veces&#39;) + theme_bw() ggplot(virusTotal, aes(x=Year, fill=country)) + geom_bar(binwidth = 0.9) + xlab(&#39;Año&#39;) + ylab(&#39;Nº de veces&#39;) + theme_bw() ## Warning: Ignoring unknown parameters: binwidth Y respecto al tipo del archivo ggplot(virusTotal, aes(x=positives, fill=file_type)) + geom_histogram(binwidth = 1) + xlab(&#39;Año&#39;) + ylab(&#39;Tipo de archivo&#39;) + theme_bw() ggplot(virusTotal, aes(x=Year, fill=file_type)) + geom_bar(binwidth = 1) + xlab(&#39;Año&#39;) + ylab(&#39;Tipo de archivo&#39;) + theme_bw() ## Warning: Ignoring unknown parameters: binwidth Y también veremos el número de positivos dependiendo del año ggplot(virusTotal, aes(x=positives, fill=Year)) + geom_histogram(binwidth = 1) + xlab(&#39;Positivos&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() Ahora usaremos Gráficos de puntos para mostrar el número de positivos por país. ggplot(data=virusTotal, aes(x=positives, y=Year)) + geom_point(aes(colour=file_type), shape=15, size=5) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() ggplot(data=virusTotal, aes(x=country, y=positives)) + geom_point(aes(colour=file_type), shape=15, size=5) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() De esta última gráfica sacamos que los archivos tipo DEX solo vienen de República Checa. ggplot(data=virusTotal, aes(x=country, y=positives)) + geom_point(aes(colour=Year), shape=15, size=5) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() Por último usaremos gráficos de barras. ggplot(virusTotal, aes(x=Year, y=positives, fill=file_type)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() ggplot(virusTotal, aes(x=Year, y=positives, fill=file_type)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() ggplot(virusTotal, aes(x=country, y=positives, fill=Year)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() library(igraph) library(curl) library(tidyjson) library(dplyr) library(purrr) library(tidyverse) library(Matrix) library(ggplot2) "],["analizando-el-ssdeep.html", "Chapter 3 Analizando el ssdeep 3.1 sample1.c 3.2 sample2.c 3.3 sample3.c 3.4 Analizando los json 3.5 Grafos 3.6 Grafo completo, G1 3.7 Grafo de distancias &lt;= 10 para G 3.8 Análisis del mayor componente de G 3.9 Analizando los resultados de los antivirus de G", " Chapter 3 Analizando el ssdeep Las funciones hash como MD5, SHA256 o otras son útiles si queremos verificar la integridad de un archivo, su principio fundamental es que un pequeño cambio en el archivo (del orden de unos pocos bits) cambia la salida drásticamente. En nuestro caso si queremos encontrar similitud entre malware no podemos usar esas funciones, porque si sabemos que un archivo es peligroso y tenemos su hash calculado, con cambiar un bit de ese archivo ya no lo podríamos detectar. Por eso existe el programa ssdeep, que permite observar pequeñas diferencias entre archivos calculando el CTPH (parecido al hash). Tenemos tres programas de ejemplo escritos en C, y queremos ver en qué porcentaje se parecen. 3.1 sample1.c #include &lt;stdio.h&gt; void main() { printf (“Hello World”); } 3.2 sample2.c #include &lt;stdio.h&gt; int main(int argc, char *argv[]) { for (int i = 0; i &lt; 100; i++) { if (i%2 == 0) { i = i + 1; } } return 0; } 3.3 sample3.c #include &lt;stdio.h&gt; void main() { int a = 5; printf (“Hello World: %d\\n”, a); } Calculamos y comparamos el ssdeep de los tres programas: $ ssdeep -s * &gt; sample_ctph.ssd $ ssdeep -m sample_ctph.ssd -s * Obtenemos lo siguiente: $ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample1 (100) $ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample2 (63) $ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample3 (80) sample1.c es mucho más parecido a sample3.c (en un 80%, mientras que solo un 63% con sample2.c). 3.4 Analizando los json De la misma manera, primero cargamos el directorio Android y el dataframe preprocesado, y luego calculamos el hash como con los ejemplos. path &lt;- &quot;~/Documentos/LCC/ProyectoVT/Proyecto/Android2/&quot; nombres_ficheros &lt;- list.files(path) df &lt;- read_csv(&quot;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotalCap2.csv&quot;) Dentro del json, hay un par clave valor que almacena el CTPH calculado (ssdeep), así que para analizarlo, escribimos primero la siguiente función que crea un dataframe con todos los json: # Entrada: json # Salida: dataframe get_ssdeep &lt;- function(x) { json &lt;- read_json(x) res &lt;- json %&gt;% gather_object() %&gt;% filter(name == &#39;ssdeep&#39;) %&gt;% as.data.frame() return (res) } # Por cada archivo en la carpeta Android # coge su ssdeep y los guarda en un dataframe df_deep &lt;- data.frame() for (i in nombres_ficheros) { df_deep &lt;- rbind(df_deep, get_ssdeep(paste0(path,i))) } # Limpia el dataframe df_deep &lt;- df_deep %&gt;% select(..JSON) colnames(df_deep) &lt;- (&#39;ssdeep&#39;) Una vez tenemos los datos, comparamos dos a dos todos los ssdeep usando la función adist y almacenamos los índices y su distancia. Índices contiene la distancia del json i con el j para luego construir una matriz. # Coge los virus cuya distancia (parecido) # en sus ssdeep sea menor que 2 # y los guarda en *indices* indices &lt;- c() for (i in 1:nrow(df_deep)) { for (j in (i+1):nrow(df_deep)) { if ( adist(df_deep[i,], df_deep[j,]) == 1) { indices &lt;- c(indices, i,j) } } } 3.4.1 Matriz de adyacencias Creamos una matriz de adyacencias con todos los json, donde en este caso la posicion Mij = 1 si la distancia entre los hashes del archivo i y el j es igual a uno. Cuanto menor es la distancia, menos diferencia hay entre los hashes y más código comparten. n &lt;- nrow(df) Mat &lt;- matrix(0, nrow = n, ncol = n) colnames(Mat) &lt;- c(1:n) row.names(Mat) &lt;- c(1:n) j &lt;- 1 while(j &lt; length(indices)) { Mat[indices[j], indices[j+1]] &lt;- 1 j &lt;- j+2 } Teniendo la matriz de adyacencias el siguiente paso es construir el grafo. Cada vértice es un json, y los arcos conectan json cuya distancia recibe por parámetro get_adj_matrix. Una vez se crea el grafo añadimos un atributo a las aristas con su distancia. # Entrada: distancia entre json # Salida: Grafo=(V,E) # V = json # E = distancias (ssdeep) get_adj_matrix &lt;- function(distancia) { indices &lt;- c() distancias &lt;- c() for (i in 1:nrow(df_deep)) { for (j in (i+1):nrow(df_deep)) { if ( adist(df_deep[i,], df_deep[j,]) &lt;= distancia) { indices &lt;- c(indices, i,j) distancias &lt;- c(distancias, adist(df_deep[i,], df_deep[j,])) } } } n &lt;- nrow(df) Mat &lt;- matrix(0, nrow = n, ncol = n) colnames(Mat) &lt;- c(1:n) row.names(Mat) &lt;- c(1:n) j &lt;- 1 while(j &lt; length(indices)) { Mat[indices[j], indices[j+1]] &lt;- 1 j &lt;- j+2 } G &lt;- graph_from_adjacency_matrix(Mat, mode = &#39;undirected&#39;) G &lt;- set_edge_attr(G, &#39;dist&#39;, value=distancias) return(G) } 3.5 Grafos Dibujamos el siguiente grafo con aquellos json cuyas distancias son menores o iguales que uno. Esto quiere decir que, por ejemplo, el grupo 147, 153 y 46 comparte gran parte de código. G &lt;- get_adj_matrix(1) Isolated = which(degree(G)==0) G2 = delete.vertices(G, Isolated) plot(G2, vertex.color=&#39;#ADD8E6&#39;, edge.curved = .1, vertex.size=20, edge.label=E(G2)$dist, vertex.frame.color = NA, layout=layout_nicely ) title(&quot;Distancias &lt;= 1&quot;,cex.main=1,col.main=&quot;Black&quot;) 3.6 Grafo completo, G1 Vamos a ver qué grafo se dibuja si restringimos menos la búsqueda y ponemos que saque todos los archivos que se parezcan como mínimo en un 60%. G1 &lt;- get_adj_matrix(60) plot(G1) title(&quot;Grafo completo con distancia &lt;= 60&quot;,cex.main=1,col.main=&quot;Black&quot;) Se pueden ver varios grupos que forman componentes y muchos otros nodos aislados. Esto puede deberse a que los componentes corresponden a alguna variante del mismo malware. Calculamos los componentes y guardamos los nodos de aquel más grande. c1 &lt;- components(G1) biggest1 &lt;- which.max(c1$csize) vids1 &lt;- V(G1)[c1$membership==biggest1] Dibujamos el subgrafo. plot(induced_subgraph(G1, vids1), edge.label=E(G1)$dist) title(&quot;Mayor componente con pesos&quot;,cex.main=1,col.main=&quot;Black&quot;) plot(induced_subgraph(G1, vids1), vertex.size=25) title(&quot;Mayor componente sin pesos&quot;,cex.main=1,col.main=&quot;Black&quot;) plot(induced_subgraph(G1, vids1),vertex.size = 5, vertex.color = &quot;#1e3f66&quot;, vertex.frame.color = &#39;red&#39;, vertex.label.cex = .7, vertex.label = NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7) title(&quot;Mayor componente&quot;,cex.main=1,col.main=&quot;Black&quot;) 3.6.1 Análisis del componente Usando el algoritmo pagerank y la función grado, ordenamos los nodos por importancia: subgrafo &lt;- induced_subgraph(G1, vids1) pg &lt;- page.rank(subgrafo) importancia &lt;- data.frame( grado = degree(subgrafo), page_rank = pg$vector ) importancia_sorted &lt;- data.frame( grado = sort(degree(subgrafo), decreasing = TRUE), page_rank = sort(pg$vector, decreasing = TRUE) ) knitr::kable(head(importancia_sorted, 10)) grado page_rank 82 62 0.0197978 163 61 0.0196663 47 60 0.0190018 124 59 0.0181018 111 58 0.0180970 154 58 0.0176471 54 57 0.0174520 39 56 0.0170076 67 56 0.0170076 115 55 0.0166772 Resaltamos los nodos con grado mayor que cincuenta: plot(subgrafo, vertex.size=ifelse(importancia[V(subgrafo),][1]&gt;50,5, 1), vertex.label=NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7) Si vemos los tamaños de los componentes, hay uno con doce nodos: c1$csize ## [1] 3 3 99 1 1 1 1 2 1 1 1 12 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [31] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 ## [61] 1 1 1 1 1 1 1 Lo dibujamos, en este caso los json comparten aproximadamente la mitad de código. vids2 &lt;- V(G1)[c1$membership==12] plot(induced_subgraph(G1, vids2), edge.label=E(G1)$dist, vertex.size=20) 3.6.2 Algunos gráficos de G1 A continuación se muestran gráficas del componente grande de G1. Transformamos las fechas a formato date para poder dibujarlas. # Comparacion del componente grande de G1 v &lt;- as(vids1, &#39;vector&#39;) df_grafo &lt;- subset(df, row.names(df) %in% v) library(plotly) df_grafo$scan_date &lt;- as.POSIXct(df_grafo$scan_date, format=&quot;%Y-%m-%d %H:%M:%S&quot;) df_grafo$first_seen &lt;- as.POSIXct(df_grafo$first_seen, format=&quot;%Y-%m-%d %H:%M:%S&quot;) # Cambiamos el nombre de la columna colnames(df_grafo)[which(colnames(df_grafo)==&#39;submission.submitter_country&#39;)] &lt;- &#39;Pais&#39; Positivos a lo largo del tiempo (fecha de escáner). p &lt;- ggplot(df_grafo, aes(x=scan_date, y=positives)) + geom_line() + ylab(&#39;Positivos&#39;) + xlab(&#39;Fecha de escáner&#39;) + theme_bw() ggplotly(p) Tamaño de los json a lo largo del tiempo (fecha de escáner). p2 &lt;- ggplot(df_grafo, aes(x=scan_date, y=size)) + geom_line() + ylab(&#39;Tamaño&#39;) + xlab(&#39;Primera vez subido&#39;) + theme_bw() ggplotly(p2) Positivos a lo largo del tiempo (primera vez que se subió). p3 &lt;- ggplot(df_grafo, aes(x=first_seen, y=positives)) + geom_line() + ylab(&#39;Positivos&#39;) + xlab(&#39;Tiempo&#39;) + theme_bw() ggplotly(p3) Regiones desde donde se subió: df_grafo %&gt;% select(times_submitted, Pais) %&gt;% group_by(Pais) %&gt;% summarise(times_submitted=sum(times_submitted)) %&gt;% ggplot(data=., aes(y=times_submitted)) + geom_bar(mapping = aes(fill=Pais, x=Pais), stat = &#39;identity&#39;) + theme_bw() Como los datos no están a escala, sumamos uno y aplicamos logaritmo para compararlos mejor: p_regiones &lt;- df_grafo %&gt;% select(times_submitted, Pais) %&gt;% group_by(Pais) %&gt;% summarise(times_submitted=sum(times_submitted)) %&gt;% ggplot(data=., aes(y=log(1+times_submitted))) + geom_bar(mapping = aes(fill=Pais, x=Pais), stat = &#39;identity&#39;) + ylab(&#39;Veces subido&#39;) + xlab(&#39;País&#39;) + theme_bw() ggplotly(p_regiones) Visualizamos el número de positivos por país: p_positivos_pais &lt;- df_grafo %&gt;% select(positives, Pais) %&gt;% group_by(Pais) %&gt;% summarise(positives=sum(positives)) %&gt;% ggplot(data=., aes(y=positives)) + geom_bar(mapping = aes(fill=Pais, x=Pais), stat = &#39;identity&#39;) + ylab(&#39;Positivos&#39;) + xlab(&#39;País&#39;) + theme_bw() ggplotly(p_positivos_pais) 3.7 Grafo de distancias &lt;= 10 para G Calculamos y dibujamos las componentes del grafo con distancias menores o iguales que diez. G &lt;- get_adj_matrix(10) c &lt;- components(G) biggest &lt;- which.max(c$csize) vids &lt;- V(G)[c$membership==biggest] plot(induced_subgraph(G, vids), edge.label=E(G)$dist) title(&quot;Distancias &lt;= 10 con pesos&quot;,cex.main=1,col.main=&quot;Black&quot;) plot(induced_subgraph(G, vids), vertex.size=25) title(&quot;Distancias &lt;= 10 sin pesos&quot;,cex.main=1,col.main=&quot;Black&quot;) plot(induced_subgraph(G, vids),vertex.size = 10, vertex.color = &quot;#1e3f66&quot;, vertex.frame.color = &#39;blue&#39;, vertex.label.cex = .7, vertex.label = NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7) title(&quot;Distancias &lt;= 10&quot;,cex.main=1,col.main=&quot;Black&quot;) 3.8 Análisis del mayor componente de G Todos los archivos tienen el mismo tamaño, subidos desde California en menos de una hora. Con una media de 20 positivos, probablemente sean el mismo archivo. v &lt;- as(vids, &#39;vector&#39;) compare &lt;- data.frame() for (i in v) { compare &lt;- rbind(compare, df[i,]) } compare %&gt;% select(size) %&gt;% unique() ## # A tibble: 1 × 1 ## size ## &lt;dbl&gt; ## 1 2669106 compare %&gt;% select(submission.submitter_country) %&gt;% unique() ## # A tibble: 1 × 1 ## submission.submitter_country ## &lt;chr&gt; ## 1 CA times &lt;- compare %&gt;% select(first_seen, scan_date) %&gt;% mutate(first_seen = gsub(&#39;20[0-9]{2}-[0-9]+-[0-9]+&#39;, &#39;&#39;, first_seen), scan_date = gsub(&#39;20[0-9]{2}-[0-9]+-[0-9]+&#39;, &#39;&#39;, scan_date)) Primera y última vez que se subió: lapply(times[,1], max) ## $first_seen ## [1] &quot; 00:46:00&quot; lapply(times[,1], min) ## $first_seen ## [1] &quot; 00:19:22&quot; Media: # Media compare %&gt;% select(positives) %&gt;% lapply(., mean) ## $positives ## [1] 19.85185 Desviación típica: # Desviación típica compare %&gt;% select(positives) %&gt;% lapply(., sd) ## $positives ## [1] 1.610153 3.9 Analizando los resultados de los antivirus de G Vamos a ver qué resultado da cada antivirus a los json (vértices) del grafo G. Si dos antivirus dan el mismo resultado en archivos diferentes que sabemos que son casi iguales es probable que compartan motor. La siguiente función recorre el directorio del dataset y crea un dataframe con los antivirus. # Entrada: Ruta al fichero # Salida: Dataframe con resultados de los AV get_results &lt;- function(json) { json_data &lt;- tidyjson::read_json(json) df_temp &lt;- json_data %&gt;% gather_object() %&gt;% filter(name==&#39;scans&#39;) %&gt;% spread_all() %&gt;% gather_object() %&gt;% select(ends_with(&#39;result&#39;)) %&gt;% .[1,] %&gt;% select(-last_col()) return(df_temp) } vids tiene los vértices de los grafos calculados. Pasamos los nombres a formato numérico con ceros a la izquierda. ficheros &lt;- sapply(vids, function(x) paste0(sprintf(&quot;%04d&quot;, x), &#39;.json&#39;) ) Creamos un dataframe y, por cada vértice, cogemos los escáneres. df_results &lt;- data.frame() for (i in ficheros) { df_results &lt;- rbind.fill(df_results, get_results(paste0(path,i))) } write.csv(df_results, &quot;~/Documentos/LCC/ProyectoVT/Proyecto/escaneres.csv&quot;) Leemos y limpiamos el dataframe. df_results &lt;- read_csv(&quot;~/Documentos/LCC/ProyectoVT/Proyecto/escaneres.csv&quot;) #df_results &lt;- df_results %&gt;% select(-..JSON) colnames(df_results) &lt;- lapply(colnames(df_results), function(x) gsub(&#39;.result&#39;, &#39;&#39;, x)) # Quitar columnas enteras NA df_results &lt;- df_results[, colSums(is.na(df_results)) != nrow(df_results)] df_results &lt;- df_results %&gt;% select(-...1) Cogemos la columna trece, que no tiene valores NA. Como en realidad todas las columnas son el mismo archivo con ver una sola nos sirve, y podemos ver cómo cada antivirus (excepto los que comparten motor) lo clasifican de manera distinta. j107 &lt;- df_results[13,] j107 &lt;- t(j107) knitr::kable(j107, col.names = c(&#39;0107.json&#39;)) 0107.json Lionic Trojan.AndroidOS.Wapnor.C!c ClamAV NA CAT-QuickHeal NA McAfee Artemis!46B876999B18 VIPRE NA Sangfor Malware.Generic-Script.Save.5b333b13 Alibaba NA K7GW Trojan ( 0051a3c71 ) Trustlook NA Arcabit NA Cyren AndroidOS/GhostPush.C.gen!Eldorado SymantecMobileInsight NA Symantec Trojan.Gen.MBT ESET-NOD32 a variant of Android/TrojanDropper.Shedun.V TrendMicro-HouseCall NA Avast Android:Revo-OU [Trj] Cynet Malicious (score: 99) Kaspersky HEUR:Trojan-Dropper.AndroidOS.Wapnor.a BitDefender NA NANO-Antivirus Trojan.Android.MLW.ebzlbe MicroWorld-eScan NA Rising NA Ad-Aware NA Emsisoft NA Comodo NA F-Secure Malware.ANDROID/Agent.hutg DrWeb Android.DownLoader.329.origin Zillya Dropper.Shedun.Android.200569 TrendMicro NA McAfee-GW-Edition Artemis!Trojan FireEye NA Sophos NA Ikarus Trojan-Dropper.AndroidOS.Shedun Avast-Mobile Android:Shedun-V [Trj] Jiangmin NA Avira ANDROID/Agent.hutg Antiy-AVL Trojan/Generic.ASBOL.A0C1 Kingsoft NA Microsoft TrojanDropper:AndroidOS/Shedun.A!MTB ZoneAlarm NA GData NA BitDefenderFalx NA AhnLab-V3 PUP/Android.Agent.839002 Tencent Dos.Trojan-dropper.Piom.Pege Yandex NA MAX malware (ai score=95) MaxSecure Android.wapnor.a Fortinet Android/Shedun.AC!tr AVG Android:Revo-OU [Trj] Si seleccionamos los AV Kasperky y ZoneAlarm se observa fácilmente que los resultados son idénticos y seguramente compartan el mismo motor. df_results %&gt;% select(Kaspersky, ZoneAlarm) %&gt;% head(., 10) %&gt;% knitr::kable(.) Kaspersky ZoneAlarm HEUR:Trojan-Dropper.AndroidOS.Hqwar.bk HEUR:Trojan-Dropper.AndroidOS.Hqwar.bk not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp NA not-a-virus:UDS:AdWare.AndroidOS.Ewind.kp not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp HEUR:Trojan-Dropper.AndroidOS.Wapnor.a NA not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp NA HEUR:Trojan-Banker.AndroidOS.Fakecalls.k HEUR:Trojan-Banker.AndroidOS.Fakecalls.k not-a-virus:HEUR:AdWare.AndroidOS.Adlo.b NA NA NA not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp HEUR:Trojan.AndroidOS.Agent.aw HEUR:Trojan.AndroidOS.Agent.aw McAfee y McAfee GW Edition también, normal porque ambos son de McAfee df_results %&gt;% select(McAfee, `McAfee-GW-Edition`) %&gt;% head(., 10) %&gt;% knitr::kable(.) McAfee McAfee-GW-Edition Artemis!BF3E6F490724 NA Artemis!919A1900C529 Artemis Artemis!919A1900C529 Artemis Artemis!1E07E4821182 Artemis!Trojan Artemis!919A1900C529 Artemis NA NA Artemis!0C5C3D793761 Artemis!PUP Artemis!4EF2BDF37187 Artemis Artemis!919A1900C529 Artemis Artemis!19295D19D0AC Artemis!Trojan "],["regresion-reglas-de-asociación-y-clustering.html", "Chapter 4 Regresion, reglas de asociación y Clustering 4.1 Regresion 4.2 Reglas de asociación 4.3 Clustering", " Chapter 4 Regresion, reglas de asociación y Clustering 4.1 Regresion library(dplyr) library(tidyverse) library(ggplot2) library(readr) virusTotal &lt;- read.csv(&#39;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotal.csv&#39;) virusTotal &lt;- virusTotal %&gt;% mutate(Year = substr(virusTotal$first_seen, 0, 4)) virusTotal &lt;- virusTotal %&gt;% rename(country = submission.submitter_country) virusTotal &lt;- virusTotal %&gt;% rename(file_type = additional_info.exiftool.FileType) Pasamos el tamaño de los archivos a MB virusTotal &lt;- virusTotal %&gt;% mutate(size = virusTotal$size/1000000) Vamos a ver todas las gráficas de nuestro dataset: plot(virusTotal) Como vemos no parece que tengamos una relación directa por parte de dos variables, hemos pensado que estaría bien analizar o ver si hay una relacion entre el tamaño que tiene un archivo y si es positivo o no, ya que podriamos pensar de que al pesar más un archivo es más propenso a traer algun tipo de software malicioso. Vamos a analizar el tamaño frente al numero de positivos para ver si existe algun tipo de correlación ggplot2::ggplot(virusTotal,aes(x=size, y=positives))+geom_point()+geom_line() f1 &lt;- lm(positives~size, data = virusTotal) plot(f1) Vemos un resumen del modelo summary(f1) ## ## Call: ## lm(formula = positives ~ size, data = virusTotal) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.8717 -2.9135 -0.8898 3.0855 15.3370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.91526 0.34738 63.088 &lt; 2e-16 *** ## size -0.06463 0.01984 -3.258 0.00134 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.45 on 181 degrees of freedom ## Multiple R-squared: 0.0554, Adjusted R-squared: 0.05018 ## F-statistic: 10.62 on 1 and 181 DF, p-value: 0.001339 De aqui podemos sacar que la correlacion es practicamente nula, solo el 5,54% de la variabilidad de los positivos tiene que ver con el tamaño, por lo tanto nuestro primer acercamiento es erróneo, y no podemos aceptar la hipótesis alternativa. No obstante, hemos dibujado la grafica y observamos que la tendencia es a tener menor numero de positivos cuanto mayor sea el tamaño del archivo analizado. Pero teniendo el cuenta los valores de R^2 ajustado y el p valor, este modelo no sirve para predecir. ggplot(virusTotal, aes(x = size, y = positives)) + geom_point() + geom_line(aes(x = size, y = predict(f1,virusTotal)),col=&quot;blue&quot;) + geom_line() 4.2 Reglas de asociación library(arules) Utilizamos apriori para buscar todas las reglas posibles con suporte &gt; 0.28 y confianza &gt; 0.27. Hemos impuesto una longitud mínima de dos para filtrar las reglas con antecedente vacío. mis_reglas &lt;- apriori(virusTotal, parameter = list(supp = 0.28818, conf = 0.27634, minlen=2)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen maxlen target ext ## 0.27634 0.1 1 none FALSE TRUE 5 0.28818 2 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 52 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[414 item(s), 183 transaction(s)] done [0.00s]. ## sorting and recoding items ... [27 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 done [0.00s]. ## writing ... [153 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. length(mis_reglas) ## [1] 153 Este es el numero de reglas que podemos encontrar en nuestro dataset El resumen de las reglas obtenidas sería el siguiente: summary(mis_reglas) ## set of 153 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 ## 70 63 20 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.000 3.000 2.673 3.000 4.000 ## ## summary of quality measures: ## support confidence coverage lift count ## Min. :0.2896 Min. :0.4240 Min. :0.2896 Min. :1.158 Min. : 53.00 ## 1st Qu.:0.3060 1st Qu.:0.9474 1st Qu.:0.3333 1st Qu.:1.500 1st Qu.: 56.00 ## Median :0.3333 Median :1.0000 Median :0.3333 Median :3.000 Median : 61.00 ## Mean :0.3362 Mean :0.9172 Mean :0.3845 Mean :2.388 Mean : 61.53 ## 3rd Qu.:0.3333 3rd Qu.:1.0000 3rd Qu.:0.3333 3rd Qu.:3.000 3rd Qu.: 61.00 ## Max. :0.6339 Max. :1.0000 Max. :0.6831 Max. :3.155 Max. :116.00 ## ## mining info: ## data ntransactions support confidence ## virusTotal 183 0.28818 0.27634 ## call ## apriori(data = virusTotal, parameter = list(supp = 0.28818, conf = 0.27634, minlen = 2)) Como podemos ver en el resumen, nos muestran datos de las reglas generadas, podemos ver por ejemplo que la media obtenida en el soporte es de 0.28818 lo cual no es muy alto e indica que la media de las reglas obtenidas no son muy frecuentes. La confianza tenemos una media de 0.27634 lo cual nos dice que la media en el dataset indica que el 27,63% de las X tambien contienen a Y. Estas son todas las reglas calculadas inspect(mis_reglas) ## lhs rhs support confidence coverage lift count ## [1] {country=CA} =&gt; {times_submitted=[1,2)} 0.3060109 1.0000000 0.3060109 1.811881 56 ## [2] {times_submitted=[1,2)} =&gt; {country=CA} 0.3060109 0.5544554 0.5519126 1.811881 56 ## [3] {country=CA} =&gt; {Year=2021} 0.3060109 1.0000000 0.3060109 1.500000 56 ## [4] {Year=2021} =&gt; {country=CA} 0.3060109 0.4590164 0.6666667 1.500000 56 ## [5] {country=CA} =&gt; {file_type=ZIP} 0.3060109 1.0000000 0.3060109 1.464000 56 ## [6] {file_type=ZIP} =&gt; {country=CA} 0.3060109 0.4480000 0.6830601 1.464000 56 ## [7] {file_type=DEX} =&gt; {country=CZ} 0.3169399 1.0000000 0.3169399 3.155172 58 ## [8] {country=CZ} =&gt; {file_type=DEX} 0.3169399 1.0000000 0.3169399 3.155172 58 ## [9] {file_type=DEX} =&gt; {times_submitted=[2,3.54e+03]} 0.2950820 0.9310345 0.3169399 2.077796 54 ## [10] {times_submitted=[2,3.54e+03]} =&gt; {file_type=DEX} 0.2950820 0.6585366 0.4480874 2.077796 54 ## [11] {country=CZ} =&gt; {times_submitted=[2,3.54e+03]} 0.2950820 0.9310345 0.3169399 2.077796 54 ## [12] {times_submitted=[2,3.54e+03]} =&gt; {country=CZ} 0.2950820 0.6585366 0.4480874 2.077796 54 ## [13] {...2=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [14] {...3=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [15] {...2=[122,183]} =&gt; {X=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [16] {X=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [17] {...2=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [18] {...1=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [19] {...3=[122,183]} =&gt; {X=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [20] {X=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [21] {...3=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [22] {...1=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [23] {X=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [24] {...1=[122,183]} =&gt; {X=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [25] {size=[3.03,178]} =&gt; {Year=2021} 0.2896175 0.8688525 0.3333333 1.303279 53 ## [26] {Year=2021} =&gt; {size=[3.03,178]} 0.2896175 0.4344262 0.6666667 1.303279 53 ## [27] {size=[3.03,178]} =&gt; {file_type=ZIP} 0.3005464 0.9016393 0.3333333 1.320000 55 ## [28] {file_type=ZIP} =&gt; {size=[3.03,178]} 0.3005464 0.4400000 0.6830601 1.320000 55 ## [29] {X=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [30] {...1=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [31] {X=[61.7,122)} =&gt; {...2=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [32] {...2=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [33] {X=[61.7,122)} =&gt; {...3=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [34] {...3=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [35] {...1=[61.7,122)} =&gt; {...2=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [36] {...2=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [37] {...1=[61.7,122)} =&gt; {...3=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [38] {...3=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [39] {...2=[61.7,122)} =&gt; {...3=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [40] {...3=[61.7,122)} =&gt; {...2=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [41] {...1=[1,61.7)} =&gt; {...2=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [42] {...2=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [43] {...1=[1,61.7)} =&gt; {...3=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [44] {...3=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [45] {...1=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [46] {X=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [47] {...2=[1,61.7)} =&gt; {...3=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [48] {...3=[1,61.7)} =&gt; {...2=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [49] {...2=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [50] {X=[1,61.7)} =&gt; {...2=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [51] {...3=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [52] {X=[1,61.7)} =&gt; {...3=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [53] {size=[1.85,3.03)} =&gt; {times_submitted=[1,2)} 0.2950820 0.8852459 0.3333333 1.603960 54 ## [54] {times_submitted=[1,2)} =&gt; {size=[1.85,3.03)} 0.2950820 0.5346535 0.5519126 1.603960 54 ## [55] {size=[1.85,3.03)} =&gt; {Year=2021} 0.3114754 0.9344262 0.3333333 1.401639 57 ## [56] {Year=2021} =&gt; {size=[1.85,3.03)} 0.3114754 0.4672131 0.6666667 1.401639 57 ## [57] {size=[1.85,3.03)} =&gt; {file_type=ZIP} 0.3005464 0.9016393 0.3333333 1.320000 55 ## [58] {file_type=ZIP} =&gt; {size=[1.85,3.03)} 0.3005464 0.4400000 0.6830601 1.320000 55 ## [59] {positives=[19,23)} =&gt; {file_type=ZIP} 0.2896175 0.7910448 0.3661202 1.158090 53 ## [60] {file_type=ZIP} =&gt; {positives=[19,23)} 0.2896175 0.4240000 0.6830601 1.158090 53 ## [61] {total=[62,64]} =&gt; {Year=2021} 0.3497268 0.8888889 0.3934426 1.333333 64 ## [62] {Year=2021} =&gt; {total=[62,64]} 0.3497268 0.5245902 0.6666667 1.333333 64 ## [63] {total=[62,64]} =&gt; {file_type=ZIP} 0.3934426 1.0000000 0.3934426 1.464000 72 ## [64] {file_type=ZIP} =&gt; {total=[62,64]} 0.3934426 0.5760000 0.6830601 1.464000 72 ## [65] {times_submitted=[1,2)} =&gt; {Year=2021} 0.5519126 1.0000000 0.5519126 1.500000 101 ## [66] {Year=2021} =&gt; {times_submitted=[1,2)} 0.5519126 0.8278689 0.6666667 1.500000 101 ## [67] {times_submitted=[1,2)} =&gt; {file_type=ZIP} 0.5300546 0.9603960 0.5519126 1.406020 97 ## [68] {file_type=ZIP} =&gt; {times_submitted=[1,2)} 0.5300546 0.7760000 0.6830601 1.406020 97 ## [69] {Year=2021} =&gt; {file_type=ZIP} 0.6338798 0.9508197 0.6666667 1.392000 116 ## [70] {file_type=ZIP} =&gt; {Year=2021} 0.6338798 0.9280000 0.6830601 1.392000 116 ## [71] {times_submitted=[1,2), ## country=CA} =&gt; {Year=2021} 0.3060109 1.0000000 0.3060109 1.500000 56 ## [72] {country=CA, ## Year=2021} =&gt; {times_submitted=[1,2)} 0.3060109 1.0000000 0.3060109 1.811881 56 ## [73] {times_submitted=[1,2), ## Year=2021} =&gt; {country=CA} 0.3060109 0.5544554 0.5519126 1.811881 56 ## [74] {times_submitted=[1,2), ## country=CA} =&gt; {file_type=ZIP} 0.3060109 1.0000000 0.3060109 1.464000 56 ## [75] {country=CA, ## file_type=ZIP} =&gt; {times_submitted=[1,2)} 0.3060109 1.0000000 0.3060109 1.811881 56 ## [76] {times_submitted=[1,2), ## file_type=ZIP} =&gt; {country=CA} 0.3060109 0.5773196 0.5300546 1.886598 56 ## [77] {country=CA, ## Year=2021} =&gt; {file_type=ZIP} 0.3060109 1.0000000 0.3060109 1.464000 56 ## [78] {country=CA, ## file_type=ZIP} =&gt; {Year=2021} 0.3060109 1.0000000 0.3060109 1.500000 56 ## [79] {file_type=ZIP, ## Year=2021} =&gt; {country=CA} 0.3060109 0.4827586 0.6338798 1.577586 56 ## [80] {country=CZ, ## file_type=DEX} =&gt; {times_submitted=[2,3.54e+03]} 0.2950820 0.9310345 0.3169399 2.077796 54 ## [81] {times_submitted=[2,3.54e+03], ## file_type=DEX} =&gt; {country=CZ} 0.2950820 1.0000000 0.2950820 3.155172 54 ## [82] {times_submitted=[2,3.54e+03], ## country=CZ} =&gt; {file_type=DEX} 0.2950820 1.0000000 0.2950820 3.155172 54 ## [83] {...2=[122,183], ## ...3=[122,183]} =&gt; {X=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [84] {X=[122,183], ## ...2=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [85] {X=[122,183], ## ...3=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [86] {...2=[122,183], ## ...3=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [87] {...1=[122,183], ## ...2=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [88] {...1=[122,183], ## ...3=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [89] {X=[122,183], ## ...2=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [90] {...1=[122,183], ## ...2=[122,183]} =&gt; {X=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [91] {X=[122,183], ## ...1=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [92] {X=[122,183], ## ...3=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [93] {...1=[122,183], ## ...3=[122,183]} =&gt; {X=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [94] {X=[122,183], ## ...1=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [95] {X=[61.7,122), ## ...1=[61.7,122)} =&gt; {...2=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [96] {X=[61.7,122), ## ...2=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [97] {...1=[61.7,122), ## ...2=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [98] {X=[61.7,122), ## ...1=[61.7,122)} =&gt; {...3=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [99] {X=[61.7,122), ## ...3=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [100] {...1=[61.7,122), ## ...3=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [101] {X=[61.7,122), ## ...2=[61.7,122)} =&gt; {...3=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [102] {X=[61.7,122), ## ...3=[61.7,122)} =&gt; {...2=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [103] {...2=[61.7,122), ## ...3=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [104] {...1=[61.7,122), ## ...2=[61.7,122)} =&gt; {...3=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [105] {...1=[61.7,122), ## ...3=[61.7,122)} =&gt; {...2=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [106] {...2=[61.7,122), ## ...3=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [107] {...1=[1,61.7), ## ...2=[1,61.7)} =&gt; {...3=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [108] {...1=[1,61.7), ## ...3=[1,61.7)} =&gt; {...2=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [109] {...2=[1,61.7), ## ...3=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [110] {...1=[1,61.7), ## ...2=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [111] {X=[1,61.7), ## ...1=[1,61.7)} =&gt; {...2=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [112] {X=[1,61.7), ## ...2=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [113] {...1=[1,61.7), ## ...3=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [114] {X=[1,61.7), ## ...1=[1,61.7)} =&gt; {...3=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [115] {X=[1,61.7), ## ...3=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [116] {...2=[1,61.7), ## ...3=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [117] {X=[1,61.7), ## ...2=[1,61.7)} =&gt; {...3=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [118] {X=[1,61.7), ## ...3=[1,61.7)} =&gt; {...2=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [119] {size=[1.85,3.03), ## times_submitted=[1,2)} =&gt; {Year=2021} 0.2950820 1.0000000 0.2950820 1.500000 54 ## [120] {size=[1.85,3.03), ## Year=2021} =&gt; {times_submitted=[1,2)} 0.2950820 0.9473684 0.3114754 1.716519 54 ## [121] {times_submitted=[1,2), ## Year=2021} =&gt; {size=[1.85,3.03)} 0.2950820 0.5346535 0.5519126 1.603960 54 ## [122] {size=[1.85,3.03), ## times_submitted=[1,2)} =&gt; {file_type=ZIP} 0.2896175 0.9814815 0.2950820 1.436889 53 ## [123] {size=[1.85,3.03), ## file_type=ZIP} =&gt; {times_submitted=[1,2)} 0.2896175 0.9636364 0.3005464 1.745995 53 ## [124] {times_submitted=[1,2), ## file_type=ZIP} =&gt; {size=[1.85,3.03)} 0.2896175 0.5463918 0.5300546 1.639175 53 ## [125] {size=[1.85,3.03), ## Year=2021} =&gt; {file_type=ZIP} 0.2950820 0.9473684 0.3114754 1.386947 54 ## [126] {size=[1.85,3.03), ## file_type=ZIP} =&gt; {Year=2021} 0.2950820 0.9818182 0.3005464 1.472727 54 ## [127] {file_type=ZIP, ## Year=2021} =&gt; {size=[1.85,3.03)} 0.2950820 0.4655172 0.6338798 1.396552 54 ## [128] {total=[62,64], ## Year=2021} =&gt; {file_type=ZIP} 0.3497268 1.0000000 0.3497268 1.464000 64 ## [129] {total=[62,64], ## file_type=ZIP} =&gt; {Year=2021} 0.3497268 0.8888889 0.3934426 1.333333 64 ## [130] {file_type=ZIP, ## Year=2021} =&gt; {total=[62,64]} 0.3497268 0.5517241 0.6338798 1.402299 64 ## [131] {times_submitted=[1,2), ## Year=2021} =&gt; {file_type=ZIP} 0.5300546 0.9603960 0.5519126 1.406020 97 ## [132] {times_submitted=[1,2), ## file_type=ZIP} =&gt; {Year=2021} 0.5300546 1.0000000 0.5300546 1.500000 97 ## [133] {file_type=ZIP, ## Year=2021} =&gt; {times_submitted=[1,2)} 0.5300546 0.8362069 0.6338798 1.515108 97 ## [134] {times_submitted=[1,2), ## country=CA, ## Year=2021} =&gt; {file_type=ZIP} 0.3060109 1.0000000 0.3060109 1.464000 56 ## [135] {times_submitted=[1,2), ## country=CA, ## file_type=ZIP} =&gt; {Year=2021} 0.3060109 1.0000000 0.3060109 1.500000 56 ## [136] {country=CA, ## file_type=ZIP, ## Year=2021} =&gt; {times_submitted=[1,2)} 0.3060109 1.0000000 0.3060109 1.811881 56 ## [137] {times_submitted=[1,2), ## file_type=ZIP, ## Year=2021} =&gt; {country=CA} 0.3060109 0.5773196 0.5300546 1.886598 56 ## [138] {X=[122,183], ## ...2=[122,183], ## ...3=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [139] {...1=[122,183], ## ...2=[122,183], ## ...3=[122,183]} =&gt; {X=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [140] {X=[122,183], ## ...1=[122,183], ## ...2=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [141] {X=[122,183], ## ...1=[122,183], ## ...3=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [142] {X=[61.7,122), ## ...1=[61.7,122), ## ...2=[61.7,122)} =&gt; {...3=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [143] {X=[61.7,122), ## ...1=[61.7,122), ## ...3=[61.7,122)} =&gt; {...2=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [144] {X=[61.7,122), ## ...2=[61.7,122), ## ...3=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [145] {...1=[61.7,122), ## ...2=[61.7,122), ## ...3=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [146] {...1=[1,61.7), ## ...2=[1,61.7), ## ...3=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [147] {X=[1,61.7), ## ...1=[1,61.7), ## ...2=[1,61.7)} =&gt; {...3=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [148] {X=[1,61.7), ## ...1=[1,61.7), ## ...3=[1,61.7)} =&gt; {...2=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [149] {X=[1,61.7), ## ...2=[1,61.7), ## ...3=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [150] {size=[1.85,3.03), ## times_submitted=[1,2), ## Year=2021} =&gt; {file_type=ZIP} 0.2896175 0.9814815 0.2950820 1.436889 53 ## [151] {size=[1.85,3.03), ## times_submitted=[1,2), ## file_type=ZIP} =&gt; {Year=2021} 0.2896175 1.0000000 0.2896175 1.500000 53 ## [152] {size=[1.85,3.03), ## file_type=ZIP, ## Year=2021} =&gt; {times_submitted=[1,2)} 0.2896175 0.9814815 0.2950820 1.778328 53 ## [153] {times_submitted=[1,2), ## file_type=ZIP, ## Year=2021} =&gt; {size=[1.85,3.03)} 0.2896175 0.5463918 0.5300546 1.639175 53 Vamos a ordenar las reglas por lift y mostramos las diez reglas con más lift mis_reglas_lift &lt;- sort(mis_reglas, by = &quot;lift&quot;) inspect(mis_reglas_lift[1:10]) ## lhs rhs support confidence ## [1] {file_type=DEX} =&gt; {country=CZ} 0.3169399 1 ## [2] {country=CZ} =&gt; {file_type=DEX} 0.3169399 1 ## [3] {times_submitted=[2,3.54e+03], file_type=DEX} =&gt; {country=CZ} 0.2950820 1 ## [4] {times_submitted=[2,3.54e+03], country=CZ} =&gt; {file_type=DEX} 0.2950820 1 ## [5] {...2=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1 ## [6] {...3=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1 ## [7] {...2=[122,183]} =&gt; {X=[122,183]} 0.3333333 1 ## [8] {X=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1 ## [9] {...2=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1 ## [10] {...1=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1 ## coverage lift count ## [1] 0.3169399 3.155172 58 ## [2] 0.3169399 3.155172 58 ## [3] 0.2950820 3.155172 54 ## [4] 0.2950820 3.155172 54 ## [5] 0.3333333 3.000000 61 ## [6] 0.3333333 3.000000 61 ## [7] 0.3333333 3.000000 61 ## [8] 0.3333333 3.000000 61 ## [9] 0.3333333 3.000000 61 ## [10] 0.3333333 3.000000 61 Vamos a ordenar las reglas por support y mostramos las diez reglas con más soporte mis_reglas_support &lt;- sort(mis_reglas, by = &quot;support&quot;) inspect(mis_reglas_support[1:10]) ## lhs rhs support confidence ## [1] {Year=2021} =&gt; {file_type=ZIP} 0.6338798 0.9508197 ## [2] {file_type=ZIP} =&gt; {Year=2021} 0.6338798 0.9280000 ## [3] {times_submitted=[1,2)} =&gt; {Year=2021} 0.5519126 1.0000000 ## [4] {Year=2021} =&gt; {times_submitted=[1,2)} 0.5519126 0.8278689 ## [5] {times_submitted=[1,2)} =&gt; {file_type=ZIP} 0.5300546 0.9603960 ## [6] {file_type=ZIP} =&gt; {times_submitted=[1,2)} 0.5300546 0.7760000 ## [7] {times_submitted=[1,2), Year=2021} =&gt; {file_type=ZIP} 0.5300546 0.9603960 ## [8] {times_submitted=[1,2), file_type=ZIP} =&gt; {Year=2021} 0.5300546 1.0000000 ## [9] {file_type=ZIP, Year=2021} =&gt; {times_submitted=[1,2)} 0.5300546 0.8362069 ## [10] {total=[62,64]} =&gt; {file_type=ZIP} 0.3934426 1.0000000 ## coverage lift count ## [1] 0.6666667 1.392000 116 ## [2] 0.6830601 1.392000 116 ## [3] 0.5519126 1.500000 101 ## [4] 0.6666667 1.500000 101 ## [5] 0.5519126 1.406020 97 ## [6] 0.6830601 1.406020 97 ## [7] 0.5519126 1.406020 97 ## [8] 0.5300546 1.500000 97 ## [9] 0.6338798 1.515108 97 ## [10] 0.3934426 1.464000 72 Como sabemos, el soporte nos indica en qué porcentaje el patrón encontrado aparece en el dataset, y vemos que cuando el año es 2021 en el 63,3% de la veces el tipo de archivo es ZIP. Vamos a ordenar las reglas por confianza y mostramos las diez reglas con más confianza mis_reglas_confidence &lt;- sort(mis_reglas, by = &quot;confidence&quot;) inspect(mis_reglas_confidence[1:10]) ## lhs rhs support confidence coverage lift count ## [1] {country=CA} =&gt; {times_submitted=[1,2)} 0.3060109 1 0.3060109 1.811881 56 ## [2] {country=CA} =&gt; {Year=2021} 0.3060109 1 0.3060109 1.500000 56 ## [3] {country=CA} =&gt; {file_type=ZIP} 0.3060109 1 0.3060109 1.464000 56 ## [4] {file_type=DEX} =&gt; {country=CZ} 0.3169399 1 0.3169399 3.155172 58 ## [5] {country=CZ} =&gt; {file_type=DEX} 0.3169399 1 0.3169399 3.155172 58 ## [6] {...2=[122,183]} =&gt; {...3=[122,183]} 0.3333333 1 0.3333333 3.000000 61 ## [7] {...3=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1 0.3333333 3.000000 61 ## [8] {...2=[122,183]} =&gt; {X=[122,183]} 0.3333333 1 0.3333333 3.000000 61 ## [9] {X=[122,183]} =&gt; {...2=[122,183]} 0.3333333 1 0.3333333 3.000000 61 ## [10] {...2=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1 0.3333333 3.000000 61 De aquí podemos sacar que el pais CA solo ha enviado archivos de tipo ZIP en 2021, o también como mostramos en la parte de visualización que República Checa siempre envia DEX como tipo de archivo. 4.3 Clustering library(magrittr) Preparamos el kmeans con 4 centros y usaremos el tamaño frente al numero de positivos set.seed(1) virus &lt;- virusTotal %&gt;% select(c(&quot;size&quot;,&quot;positives&quot;))%&gt;% kmeans(centers=4, nstart=10) str(virus) ## List of 9 ## $ cluster : int [1:183] 4 4 4 2 4 4 4 4 4 2 ... ## $ centers : num [1:4, 1:2] 178.36 1.76 74.81 4.49 15 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. ..$ : chr [1:2] &quot;size&quot; &quot;positives&quot; ## $ totss : num 54130 ## $ withinss : num [1:4] 0 912 1675 3606 ## $ tot.withinss: num 6193 ## $ betweenss : num 47937 ## $ size : int [1:4] 1 65 3 114 ## $ iter : int 2 ## $ ifault : int 0 ## - attr(*, &quot;class&quot;)= chr &quot;kmeans&quot; Vemos el tamaño de los clusters y de los centroides virus$size ## [1] 1 65 3 114 virus$centers ## size positives ## 1 178.355426 15.00000 ## 2 1.761319 26.61538 ## 3 74.811670 16.00000 ## 4 4.487114 18.86842 Ahora vamos a visualizar un grafico del tamaño (eje X) frente al numero de positivos (eje Y), lo dibujaremos con colores según el cluster al que pertenezca y la etiqueta será el tipo de archivo analizado. Como la gráfica quedaba muy comprimida la vamos a mostrar por partes para un mejor entendimiento plot(virusTotal$size,virusTotal$positives, type=&quot;n&quot;,xlim = c(0,10), xlab = &quot;size&quot;,ylab = &quot;positives&quot;) text(x=virusTotal$size, y=virusTotal$positives, labels=virusTotal$file_type, col=virus$cluster+1) Aqui vemos los archivos de entre 0 y 10 MB que es donde se concentran la mayoría plot(virusTotal$size,virusTotal$positives, type=&quot;n&quot;,xlim = c(11,100), xlab = &quot;size&quot;,ylab = &quot;positives&quot;) text(x=virusTotal$size, y=virusTotal$positives, labels=virusTotal$file_type, col=virus$cluster+1) Aqui los archivos entre 11 y 100 MB que aunque haya menos tambien podemos ver los clusters en los que se agrupan plot(virusTotal$size,virusTotal$positives, type=&quot;n&quot;, xlab = &quot;size&quot;,ylab = &quot;positives&quot;) text(x=virusTotal$size, y=virusTotal$positives, labels=virusTotal$file_type, col=virus$cluster+1) Y por ultimo aqui ya vemos la gráfica al completo "],["footnotes-and-citations.html", "Chapter 5 Footnotes and citations 5.1 Footnotes 5.2 Citations", " Chapter 5 Footnotes and citations 5.1 Footnotes Footnotes are put inside the square brackets after a caret ^[]. Like this one 1. 5.2 Citations Reference items in your bibliography file(s) using @key. For example, we are using the bookdown package (Xie 2022) (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) (this citation was added manually in an external file book.bib). Note that the .bib files need to be listed in the index.Rmd with the YAML bibliography key. The RStudio Visual Markdown Editor can also make it easier to insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations References "],["blocks.html", "Chapter 6 Blocks 6.1 Equations 6.2 Theorems and proofs 6.3 Callout blocks", " Chapter 6 Blocks 6.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{6.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (6.1). 6.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 6.1. Theorem 6.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 6.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html "],["sharing-your-book.html", "Chapter 7 Sharing your book 7.1 Publishing 7.2 404 pages 7.3 Metadata for sharing", " Chapter 7 Sharing your book 7.1 Publishing HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html 7.2 404 pages By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you’d like to customize your 404 page instead of using the default, you may add either a _404.Rmd or _404.md file to your project root and use code and/or Markdown syntax. 7.3 Metadata for sharing Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the index.Rmd YAML. To setup, set the url for your book and the path to your cover-image file. Your book’s title and description are also used. This gitbook uses the same social sharing data across all chapters in your book- all links shared will look the same. Specify your book’s source repository on GitHub using the edit key under the configuration options in the _output.yml file, which allows users to suggest an edit by linking to a chapter’s source file. Read more about the features of this output format here: https://pkgs.rstudio.com/bookdown/reference/gitbook.html Or use: ?bookdown::gitbook "],["references.html", "References", " References "]]
