[["index.html", "Virus Total Chapter 1 Preprocesado de datos 1.1 Inicio 1.2 Renombrando los archivos 1.3 Datos iniciales 1.4 Preprocesado", " Virus Total Samuel Sánchez Toca y Alejandro Medina Astorga 2022-06-09 Chapter 1 Preprocesado de datos Este es el proyecto realizado por Samuel Sánchez Toca y Alejandro Medina Astorga para la asignatura Laboratorio de Computación Científica para el dataset VirusTotal. 1.1 Inicio Primero tuvimos que preprocesar los datos que venían en formato json, por lo que tuvimos que extraerlos y decidir qué información iba a resultar más útil para su analisis. Para el preprocesado, hemos usado las librerías jsonlite, tidyjson, jsonlite y tidyverse para manejar los archivos. El dataset consta de casi doscientos ficheros con la información obtenida al analizarlos con distintos antivirus, el primer paso es, por tanto, ver cómo están estructurados los datos para decidir con qué información nos vamos a quedar. 1.2 Renombrando los archivos El siguiente script cambia los nombres a los archivos para facilitar su uso para hacer pruebas: a=1 for i in *.json; do new=$(printf &quot;%04d.json&quot; &quot;$a&quot;) mv -i -- &quot;$i&quot; &quot;$new&quot; let a=a+1 done 1.3 Datos iniciales Primero, listamos los nombres de todos los archivos y los guardamos en la variable nombres_ficheros: path &lt;- &quot;~/Documentos/LCC/ProyectoVT/Proyecto/Android2&quot; nombres_ficheros &lt;- list.files(path) Luego empezamos importando el primer fichero para ver su estructura y explorarlo para tener una idea de cómo comenzar el preprocesado y el análisis. Usando la función read_json de la librería tidyjson leemos el fichero y lo guardamos en una variable: json_0001 &lt;- tidyjson::read_json(paste0(path, &quot;/0001.json&quot;)) 1.4 Preprocesado La función que hemos usado para construir un dataframe es spread_all, que de forma recursiva convierte todos los clave valor del json en columnas. library(curl) library(tidyjson) library(dplyr) library(purrr) library(tidyverse) library(kableExtra) library(bookdown) json_data &lt;- tidyjson::read_json(&quot;~/Documentos/LCC/ProyectoVT/Proyecto/Android2/0001.json&quot;) tbl &lt;- json_data %&gt;% spread_all() tbl ## # A tbl_json: 1 x 337 tibble with a &quot;JSON&quot; attribute ## ..JSON document.id vhash scan_date first_seen total size authentihash times_submitted ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 &quot;{\\&quot;vh… 1 f6e2… 2021-11-… 2021-11-0… 61 1.96e6 NA 1 ## # … with 328 more variables: harmless_votes &lt;dbl&gt;, malicious_votes &lt;dbl&gt;, sha256 &lt;chr&gt;, ## # type &lt;chr&gt;, scan_id &lt;chr&gt;, unique_sources &lt;dbl&gt;, positives &lt;dbl&gt;, ssdeep &lt;chr&gt;, ## # md5 &lt;chr&gt;, permalink &lt;chr&gt;, sha1 &lt;chr&gt;, response_code &lt;dbl&gt;, ## # community_reputation &lt;dbl&gt;, verbose_msg &lt;chr&gt;, last_seen &lt;chr&gt;, ## # additional_info.trid &lt;chr&gt;, additional_info.magic &lt;chr&gt;, ## # submission.submitter_region &lt;chr&gt;, submission.date &lt;chr&gt;, ## # submission.submitter_country &lt;chr&gt;, submission.filename &lt;chr&gt;, … Ahora vamos a hacer lo mismo pero con todos los ficheros de la carpeta: #path &lt;- &quot;~/GitHub/VirusTotal/ProyectoVT/Proyecto/Android2&quot; path &lt;- &quot;~/Documentos/LCC/ProyectoVT/Proyecto/Android2&quot; nombres_ficheros &lt;- list.files(path) j &lt;- 0 df &lt;- data.frame() for (i in nombres_ficheros) { filepath &lt;- file.path(path, paste0(i)) i &lt;- tidyjson::read_json(filepath) tbl &lt;- i %&gt;% spread_all(recursive = TRUE) nombre_l &lt;- tbl %&gt;% select(scan_date,first_seen,total,size,times_submitted,positives, submission.submitter_country,additional_info.exiftool.FileType) name &lt;- as.data.frame(nombre_l) name &lt;- name[-length(name)] df &lt;- rbind(df, name) j &lt;- j+1 } Con esto hemos preprocesado todos los archivos json y los metemos en un data frame. Hemos seleccionado las columnas scan_date, first_seen, total, size, times_submitted, country, fileType. Finalmente, mostramos el dataset resultante y lo guardamos como un .csv para tenerlo a mano. df %&gt;% head(10) %&gt;% kable(., booktabs = TRUE) #%&gt;% kable_styling(font_size = 10) …1 scan_date first_seen total size times_submitted positives submission.submitter_country additional_info.exiftool.FileType 1 2021-11-03 08:53:57 2021-11-03 08:53:57 61 1956125 1 20 US ZIP 2 2021-11-03 00:26:03 2021-11-03 00:26:03 61 2667641 1 20 CA ZIP 3 2021-11-03 08:12:06 2021-11-03 08:12:06 61 3998656 1 18 UA ZIP 4 2021-11-03 08:44:36 2015-01-20 23:53:18 64 500276 263 26 IE ZIP 5 2021-11-03 08:54:32 2021-11-03 08:54:32 62 1956125 1 20 US ZIP 6 2021-11-03 08:22:50 2021-11-03 08:22:50 61 4137920 1 18 UA ZIP 7 2021-11-03 07:43:10 2021-06-03 18:29:36 61 3031864 2 15 UA ZIP 8 2021-11-03 00:25:19 2021-11-03 00:25:19 60 2669106 1 20 CA ZIP 9 2021-11-03 00:19:30 2021-11-03 00:19:30 60 2669106 1 21 CA ZIP 10 2021-11-03 08:00:30 2017-11-28 15:53:18 60 5884 4 25 CZ DEX #write.csv(df,&quot;~/GitHub/VirusTotal/ProyectoVT/Proyecto/virusTotal.csv&quot;, row.names = FALSE)## Importar dataset write.csv(df, &quot;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotal.csv&quot;) write.csv(df, &quot;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotalCap2.csv&quot;) "],["analisis-exploratorio.html", "Chapter 2 Analisis exploratorio 2.1 Dplyr 2.2 Visualizacion", " Chapter 2 Analisis exploratorio 2.1 Dplyr Primero cargamos las librerias y leemos el csv que anteriormente generamos library(dplyr) library(tidyverse) library(kableExtra) #virusTotal &lt;- read.csv(&#39;~/GitHub/VirusTotal/ProyectoVT/Proyecto/virusTotal.csv&#39;) virusTotal &lt;- read.csv(&#39;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotal.csv&#39;) Vamos a añadir una columna year y cambiamos los nombres de algunas columnas para que sean más legibles. virusTotal &lt;- virusTotal %&gt;% mutate(Year = substr(virusTotal$first_seen, 0, 4)) virusTotal &lt;- virusTotal %&gt;% rename(country = submission.submitter_country) virusTotal &lt;- virusTotal %&gt;% rename(file_type = additional_info.exiftool.FileType) kable(head(virusTotal,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) X …1 scan_date first_seen total size times_submitted positives country file_type Year 1 1 2021-11-03 08:53:57 2021-11-03 08:53:57 61 1956125 1 20 US ZIP 2021 2 2 2021-11-03 00:26:03 2021-11-03 00:26:03 61 2667641 1 20 CA ZIP 2021 3 3 2021-11-03 08:12:06 2021-11-03 08:12:06 61 3998656 1 18 UA ZIP 2021 4 4 2021-11-03 08:44:36 2015-01-20 23:53:18 64 500276 263 26 IE ZIP 2015 5 5 2021-11-03 08:54:32 2021-11-03 08:54:32 62 1956125 1 20 US ZIP 2021 6 6 2021-11-03 08:22:50 2021-11-03 08:22:50 61 4137920 1 18 UA ZIP 2021 7 7 2021-11-03 07:43:10 2021-06-03 18:29:36 61 3031864 2 15 UA ZIP 2021 8 8 2021-11-03 00:25:19 2021-11-03 00:25:19 60 2669106 1 20 CA ZIP 2021 9 9 2021-11-03 00:19:30 2021-11-03 00:19:30 60 2669106 1 21 CA ZIP 2021 10 10 2021-11-03 08:00:30 2017-11-28 15:53:18 60 5884 4 25 CZ DEX 2017 Vamos a encontrar los 10 archivos más pesados del dataset virusTotal1 &lt;- virusTotal %&gt;% arrange(desc(size)) %&gt;% slice(1:10) kable(head(virusTotal1,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) X …1 scan_date first_seen total size times_submitted positives country file_type Year 85 85 2021-11-03 08:30:37 2021-11-03 08:30:37 61 178355426 1 15 RU ZIP 2021 64 64 2021-11-03 07:37:40 2021-11-03 07:37:40 60 107759215 1 17 US ZIP 2021 50 50 2021-11-03 07:33:53 2019-09-08 03:35:40 62 63071192 7 15 GB ZIP 2019 103 103 2021-11-03 09:06:44 2018-06-30 02:26:29 63 53604603 3 16 FR ZIP 2018 34 34 2021-11-03 08:16:18 2021-11-03 08:16:18 61 30711904 1 18 CA ZIP 2021 105 105 2021-11-03 08:17:09 2021-11-03 08:17:09 63 21035680 1 23 CA ZIP 2021 16 16 2021-11-03 08:16:40 2021-11-03 08:16:40 63 20353542 1 22 CA ZIP 2021 49 49 2021-11-03 09:14:16 2021-11-03 09:14:16 60 15721098 1 16 KR ZIP 2021 175 175 2021-11-03 08:23:25 2021-11-03 08:23:25 63 15645802 1 16 NA ZIP 2021 58 58 2021-11-03 08:03:59 2021-11-03 08:03:59 63 15410862 1 18 KR ZIP 2021 Pasamos a ordenar el dataset dependiendo del tipo del archivo virusTotal2 &lt;- virusTotal %&gt;% arrange(file_type) kable(head(virusTotal2,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) X …1 scan_date first_seen total size times_submitted positives country file_type Year 10 10 2021-11-03 08:00:30 2017-11-28 15:53:18 60 5884 4 25 CZ DEX 2017 15 15 2021-11-03 09:16:16 2018-07-07 07:04:03 59 307908 11 20 CZ DEX 2018 20 20 2021-11-03 09:00:20 2019-10-01 03:13:27 56 48204 2 20 CZ DEX 2019 21 21 2021-11-03 07:50:10 2016-12-03 11:18:21 53 366676 2 21 CZ DEX 2016 23 23 2021-11-03 07:40:40 2015-04-07 10:32:52 59 1674660 20 25 CZ DEX 2015 24 24 2021-11-03 08:46:27 2017-10-27 21:12:27 59 424720 2 20 CZ DEX 2017 26 26 2021-11-03 07:39:47 2018-04-16 22:03:40 59 5888 2 25 CZ DEX 2018 27 27 2021-11-03 07:39:19 2019-09-01 04:43:45 60 7116 2 17 CZ DEX 2019 28 28 2021-11-03 08:00:41 2017-08-18 16:45:45 59 17476 3 25 CZ DEX 2017 30 30 2021-11-03 07:39:44 2018-04-03 07:07:46 59 6004 3 26 CZ DEX 2018 Mostrar los archivos que son más pesados que la media mediaSize &lt;- mean(virusTotal$size) mediaSize ## [1] 5621897 virusTotal3 &lt;- virusTotal %&gt;% filter(virusTotal$size &gt; mediaSize)%&gt;% arrange(size) kable(head(virusTotal3,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) X …1 scan_date first_seen total size times_submitted positives country file_type Year 110 110 2021-11-03 07:52:59 2021-10-20 00:56:21 60 6122851 4 19 IT ZIP 2021 17 17 2021-11-03 07:54:29 2021-10-18 08:50:50 62 6122860 2 18 IT ZIP 2021 70 70 2021-11-03 07:55:40 2021-10-19 04:55:25 62 6122862 2 23 IT ZIP 2021 38 38 2021-11-03 07:54:48 2021-10-18 02:40:12 62 6122893 2 21 IT ZIP 2021 156 156 2021-11-03 07:56:23 2021-10-19 13:37:27 62 6122901 2 26 IT ZIP 2021 161 161 2021-11-03 07:52:28 2021-10-19 13:53:09 62 6122902 2 27 IT ZIP 2021 43 43 2021-11-03 07:54:09 2021-10-21 10:15:18 61 6126959 2 20 IT ZIP 2021 29 29 2021-11-03 07:40:00 2021-01-16 23:49:02 63 7032582 255 19 MX ZIP 2021 63 63 2021-11-03 07:38:33 2019-05-13 04:00:23 59 7108008 3 16 CZ DEX 2019 152 152 2021-11-03 09:12:48 2021-10-20 08:44:12 57 7225742 2 21 US ZIP 2021 Buscamos el primer archivo registrado virusTotal4 &lt;- virusTotal %&gt;% arrange(first_seen)%&gt;% select(first_seen,Year,size,times_submitted,positives,file_type)%&gt;% slice(1) kable(head(virusTotal4,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) first_seen Year size times_submitted positives file_type 2012-12-18 03:26:17 2012 6392 6 28 DEX Por último vamos a contar los archivos dependiendo de su tipo virusTotal5 &lt;- virusTotal%&gt;% group_by(file_type)%&gt;% summarise( n = n() ) kable(head(virusTotal5,10), booktabs = TRUE) %&gt;% kable_styling(font_size = 10) file_type n DEX 58 ZIP 125 2.2 Visualizacion Vamos a analizar el dataset a traves de los gráficos. Empezamos con el numero de positivos y las veces que se repiten con Histogramas. library(ggplot2) ggplot(virusTotal, aes(x=positives)) + geom_histogram(binwidth = 0.5) + xlab(&#39;Positivos&#39;) + ylab(&#39;Nº de veces&#39;) + theme_bw() Aquí vemos una gráfica de los años según las veces que aparecen en el dataset ggplot(virusTotal, aes(x=Year)) + geom_bar(binwidth = 0.9) + xlab(&#39;Año&#39;) + ylab(&#39;Nº de veces&#39;) + theme_bw() Ahora visualizaremos lo anterior pero con los colores dependiendo del país ggplot(virusTotal, aes(x=positives, fill=country)) + geom_histogram(binwidth = 0.5) + xlab(&#39;Año&#39;) + ylab(&#39;Positivos&#39;) + theme_bw() ggplot(virusTotal, aes(x=Year, fill=country)) + geom_bar(binwidth = 0.9) + xlab(&#39;Año&#39;) + ylab(&#39;País&#39;) + theme_bw() Y respecto al tipo del archivo ggplot(virusTotal, aes(x=positives, fill=file_type)) + geom_histogram(binwidth = 1) + xlab(&#39;Positivos&#39;) + ylab(&#39;Tipo de archivo&#39;) + theme_bw() ggplot(virusTotal, aes(x=Year, fill=file_type)) + geom_bar(binwidth = 1) + xlab(&#39;Año&#39;) + ylab(&#39;Tipo de archivo&#39;) + theme_bw() Y también veremos el número de positivos dependiendo del año ggplot(virusTotal, aes(x=positives, fill=Year)) + geom_histogram(binwidth = 1) + xlab(&#39;Positivos&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() Ahora usaremos Gráficos de puntos para mostrar el número de positivos por país. ggplot(data=virusTotal, aes(x=positives, y=Year)) + geom_point(aes(colour=file_type), shape=15, size=5) + xlab(&#39;Nº positivos&#39;) + ylab(&#39;Año&#39;) + theme_bw() ggplot(data=virusTotal, aes(x=country, y=positives)) + geom_point(aes(colour=file_type), shape=15, size=5) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() De esta última gráfica sacamos que los archivos tipo DEX solo vienen de República Checa. ggplot(data=virusTotal, aes(x=country, y=positives)) + geom_point(aes(colour=Year), shape=15, size=5) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() Por último usaremos gráficos de barras. ggplot(virusTotal, aes(x=Year, y=positives, fill=file_type)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + xlab(&#39;Año&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() ggplot(virusTotal, aes(x=Year, y=positives, fill=file_type)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + xlab(&#39;Año&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() ggplot(virusTotal, aes(x=country, y=positives, fill=Year)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + xlab(&#39;País&#39;) + ylab(&#39;Nº positivos&#39;) + theme_bw() library(igraph) library(curl) library(tidyjson) library(dplyr) library(purrr) library(tidyverse) library(Matrix) library(ggplot2) "],["analizando-el-ssdeep.html", "Chapter 3 Analizando el ssdeep 3.1 sample1.c 3.2 sample2.c 3.3 sample3.c 3.4 Analizando los json 3.5 Grafos 3.6 Grafo completo, G1 3.7 Grafo de distancias &lt;= 10 para G 3.8 Análisis del mayor componente de G 3.9 Analizando los resultados de los antivirus de G", " Chapter 3 Analizando el ssdeep Las funciones hash como MD5, SHA256 o otras son útiles si queremos verificar la integridad de un archivo, su principio fundamental es que un pequeño cambio en el archivo (del orden de unos pocos bits) cambia la salida drásticamente. En nuestro caso si queremos encontrar similitud entre malware no podemos usar esas funciones, porque si sabemos que un archivo es peligroso y tenemos su hash calculado, con cambiar un bit de ese archivo ya no lo podríamos detectar. Por eso existe el programa ssdeep, que permite observar pequeñas diferencias entre archivos calculando el CTPH (parecido al hash). Tenemos tres programas de ejemplo escritos en C, y queremos ver en qué porcentaje se parecen. 3.1 sample1.c #include &lt;stdio.h&gt; void main() { printf (“Hello World”); } 3.2 sample2.c #include &lt;stdio.h&gt; int main(int argc, char *argv[]) { for (int i = 0; i &lt; 100; i++) { if (i%2 == 0) { i = i + 1; } } return 0; } 3.3 sample3.c #include &lt;stdio.h&gt; void main() { int a = 5; printf (“Hello World: %d\\n”, a); } Calculamos y comparamos el ssdeep de los tres programas: $ ssdeep -s * &gt; sample_ctph.ssd $ ssdeep -m sample_ctph.ssd -s * Obtenemos lo siguiente: $ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample1 (100) $ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample2 (63) $ sample1 matches sample_ctph.ssd:/home/samuel/Documentos/LCC/pruebas/sample3 (80) sample1.c es mucho más parecido a sample3.c (en un 80%, mientras que solo un 63% con sample2.c). 3.4 Analizando los json De la misma manera, primero cargamos el directorio Android y el dataframe preprocesado, y luego calculamos el hash como con los ejemplos. path &lt;- &quot;~/Documentos/LCC/ProyectoVT/Proyecto/Android2/&quot; nombres_ficheros &lt;- list.files(path) df &lt;- read_csv(&quot;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotalCap2.csv&quot;) Dentro del json, hay un par clave valor que almacena el CTPH calculado (ssdeep), así que para analizarlo, escribimos primero la siguiente función que crea un dataframe con todos los json: # Entrada: json # Salida: dataframe get_ssdeep &lt;- function(x) { json &lt;- read_json(x) res &lt;- json %&gt;% gather_object() %&gt;% filter(name == &#39;ssdeep&#39;) %&gt;% as.data.frame() return (res) } # Por cada archivo en la carpeta Android # coge su ssdeep y los guarda en un dataframe df_deep &lt;- data.frame() for (i in nombres_ficheros) { df_deep &lt;- rbind(df_deep, get_ssdeep(paste0(path,i))) } # Limpia el dataframe df_deep &lt;- df_deep %&gt;% select(..JSON) colnames(df_deep) &lt;- (&#39;ssdeep&#39;) Una vez tenemos los datos, comparamos dos a dos todos los ssdeep usando la función adist y almacenamos los índices y su distancia. Índices contiene la distancia del json i con el j para luego construir una matriz. # Coge los virus cuya distancia (parecido) # en sus ssdeep sea menor que 2 # y los guarda en *indices* indices &lt;- c() for (i in 1:nrow(df_deep)) { for (j in (i+1):nrow(df_deep)) { if ( adist(df_deep[i,], df_deep[j,]) == 1) { indices &lt;- c(indices, i,j) } } } 3.4.1 Matriz de adyacencias Creamos una matriz de adyacencias con todos los json, donde en este caso la posicion Mij = 1 si la distancia entre los hashes del archivo i y el j es igual a uno. Cuanto menor es la distancia, menos diferencia hay entre los hashes y más código comparten. n &lt;- nrow(df) Mat &lt;- matrix(0, nrow = n, ncol = n) colnames(Mat) &lt;- c(1:n) row.names(Mat) &lt;- c(1:n) j &lt;- 1 while(j &lt; length(indices)) { Mat[indices[j], indices[j+1]] &lt;- 1 j &lt;- j+2 } Teniendo la matriz de adyacencias el siguiente paso es construir el grafo. Cada vértice es un json, y los arcos conectan json cuya distancia recibe por parámetro get_adj_matrix. Una vez se crea el grafo añadimos un atributo a las aristas con su distancia. # Entrada: distancia entre json # Salida: Grafo=(V,E) # V = json # E = distancias (ssdeep) get_adj_matrix &lt;- function(distancia) { indices &lt;- c() distancias &lt;- c() for (i in 1:nrow(df_deep)) { for (j in (i+1):nrow(df_deep)) { if ( adist(df_deep[i,], df_deep[j,]) &lt;= distancia) { indices &lt;- c(indices, i,j) distancias &lt;- c(distancias, adist(df_deep[i,], df_deep[j,])) } } } n &lt;- nrow(df) Mat &lt;- matrix(0, nrow = n, ncol = n) colnames(Mat) &lt;- c(1:n) row.names(Mat) &lt;- c(1:n) j &lt;- 1 while(j &lt; length(indices)) { Mat[indices[j], indices[j+1]] &lt;- 1 j &lt;- j+2 } G &lt;- graph_from_adjacency_matrix(Mat, mode = &#39;undirected&#39;) G &lt;- set_edge_attr(G, &#39;dist&#39;, value=distancias) return(G) } 3.5 Grafos Dibujamos el siguiente grafo con aquellos json cuyas distancias son menores o iguales que uno. Esto quiere decir que, por ejemplo, el grupo 147, 153 y 46 comparte gran parte de código. G &lt;- get_adj_matrix(1) Isolated = which(degree(G)==0) G2 = delete.vertices(G, Isolated) plot(G2, vertex.color=&#39;#ADD8E6&#39;, edge.curved = .1, vertex.size=20, edge.label=E(G2)$dist, vertex.frame.color = NA, layout=layout_nicely ) title(&quot;Distancias &lt;= 1&quot;,cex.main=1,col.main=&quot;Black&quot;) Construimos un dataframe con los nodos para analizarlos vertices &lt;- V(G2)$name df_1 &lt;- subset(df, row.names(df) %in% vertices) Información sobre los nodos df_1 %&gt;% select(submission.submitter_country) %&gt;% unique() ## # A tibble: 1 × 1 ## submission.submitter_country ## &lt;chr&gt; ## 1 CA df_1 %&gt;% select(scan_date) ## # A tibble: 14 × 1 ## scan_date ## &lt;dttm&gt; ## 1 2021-11-03 00:19:30 ## 2 2021-11-03 00:19:22 ## 3 2021-11-03 00:27:26 ## 4 2021-11-03 00:25:46 ## 5 2021-11-03 00:19:33 ## 6 2021-11-03 00:27:35 ## 7 2021-11-03 00:31:34 ## 8 2021-11-03 00:25:15 ## 9 2021-11-03 00:19:41 ## 10 2021-11-03 00:21:08 ## 11 2021-11-03 00:26:11 ## 12 2021-11-03 00:20:03 ## 13 2021-11-03 00:36:04 ## 14 2021-11-03 08:46:17 df_1 %&gt;% select(positives) ## # A tibble: 14 × 1 ## positives ## &lt;dbl&gt; ## 1 21 ## 2 19 ## 3 17 ## 4 21 ## 5 17 ## 6 21 ## 7 21 ## 8 21 ## 9 19 ## 10 20 ## 11 22 ## 12 20 ## 13 20 ## 14 19 df_1 %&gt;% select(size) %&gt;% unique() ## # A tibble: 2 × 1 ## size ## &lt;dbl&gt; ## 1 2669106 ## 2 2369760 df_1 %&gt;% select(times_submitted) %&gt;% unique() ## # A tibble: 1 × 1 ## times_submitted ## &lt;dbl&gt; ## 1 1 df_1 %&gt;% select(first_seen) ## # A tibble: 14 × 1 ## first_seen ## &lt;dttm&gt; ## 1 2021-11-03 00:19:30 ## 2 2021-11-03 00:19:22 ## 3 2021-11-03 00:27:26 ## 4 2021-11-03 00:25:46 ## 5 2021-11-03 00:19:33 ## 6 2021-11-03 00:27:35 ## 7 2021-11-03 00:31:34 ## 8 2021-11-03 00:25:15 ## 9 2021-11-03 00:19:41 ## 10 2021-11-03 00:21:08 ## 11 2021-11-03 00:26:11 ## 12 2021-11-03 00:20:03 ## 13 2021-11-03 00:36:04 ## 14 2021-11-03 08:46:17 3.6 Grafo completo, G1 Vamos a ver qué grafo se dibuja si restringimos menos la búsqueda y ponemos que saque todos los archivos que se parezcan como mínimo en un 60%. G1 &lt;- get_adj_matrix(60) plot(G1) title(&quot;Grafo completo con distancia &lt;= 60&quot;,cex.main=1,col.main=&quot;Black&quot;) Se pueden ver varios grupos que forman componentes y muchos otros nodos aislados. Esto puede deberse a que los componentes corresponden a alguna variante del mismo malware. Calculamos los componentes y guardamos los nodos de aquel más grande. c1 &lt;- components(G1) biggest1 &lt;- which.max(c1$csize) vids1 &lt;- V(G1)[c1$membership==biggest1] Dibujamos el subgrafo. plot(induced_subgraph(G1, vids1), edge.label=E(G1)$dist) title(&quot;Mayor componente con pesos&quot;,cex.main=1,col.main=&quot;Black&quot;) plot(induced_subgraph(G1, vids1), vertex.size=25) title(&quot;Mayor componente sin pesos&quot;,cex.main=1,col.main=&quot;Black&quot;) plot(induced_subgraph(G1, vids1),vertex.size = 5, vertex.color = &quot;#1e3f66&quot;, vertex.frame.color = &#39;red&#39;, vertex.label.cex = .7, vertex.label = NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7) title(&quot;Mayor componente&quot;,cex.main=1,col.main=&quot;Black&quot;) 3.6.1 Análisis del componente Usando el algoritmo pagerank y la función grado, ordenamos los nodos por importancia: subgrafo &lt;- induced_subgraph(G1, vids1) pg &lt;- page.rank(subgrafo) importancia &lt;- data.frame( grado = degree(subgrafo), page_rank = pg$vector ) importancia_sorted &lt;- data.frame( grado = sort(degree(subgrafo), decreasing = TRUE), page_rank = sort(pg$vector, decreasing = TRUE) ) knitr::kable(head(importancia_sorted, 10)) grado page_rank 82 62 0.0197978 163 61 0.0196663 47 60 0.0190018 124 59 0.0181018 111 58 0.0180970 154 58 0.0176471 54 57 0.0174520 39 56 0.0170076 67 56 0.0170076 115 55 0.0166772 Resaltamos los nodos con grado mayor que cincuenta: plot(subgrafo, vertex.size=ifelse(importancia[V(subgrafo),][1]&gt;50,5, 1), vertex.label=NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7) Si vemos los tamaños de los componentes, hay uno con doce nodos: c1$csize ## [1] 3 3 99 1 1 1 1 2 1 1 1 12 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [29] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 ## [57] 1 1 1 1 1 1 1 1 1 1 1 Lo dibujamos, en este caso los json comparten aproximadamente la mitad de código. vids2 &lt;- V(G1)[c1$membership==12] plot(induced_subgraph(G1, vids2), edge.label=E(G1)$dist, vertex.size=20) 3.6.2 Algunos gráficos de G1 A continuación se muestran gráficas del componente grande de G1. Transformamos las fechas a formato date para poder dibujarlas. # Comparacion del componente grande de G1 v &lt;- as(vids1, &#39;vector&#39;) df_grafo &lt;- subset(df, row.names(df) %in% v) library(plotly) df_grafo$scan_date &lt;- as.POSIXct(df_grafo$scan_date, format=&quot;%Y-%m-%d %H:%M:%S&quot;) df_grafo$first_seen &lt;- as.POSIXct(df_grafo$first_seen, format=&quot;%Y-%m-%d %H:%M:%S&quot;) # Cambiamos el nombre de la columna colnames(df_grafo)[which(colnames(df_grafo)==&#39;submission.submitter_country&#39;)] &lt;- &#39;Pais&#39; Se subieron en un intervalo de nueve horas max(df_grafo$scan_date)-min(df_grafo$scan_date) ## Time difference of 9.341389 hours Positivos a lo largo del tiempo (fecha de escáner). p &lt;- ggplot(df_grafo, aes(x=scan_date, y=positives)) + geom_line() + ylab(&#39;Positivos&#39;) + xlab(&#39;Fecha de escáner&#39;) + theme_bw() ggplotly(p) Tamaño de los json a lo largo del tiempo (fecha de escáner). p2 &lt;- ggplot(df_grafo, aes(x=scan_date, y=size)) + geom_line() + ylab(&#39;Tamaño&#39;) + xlab(&#39;Primera vez subido&#39;) + theme_bw() ggplotly(p2) Positivos a lo largo del tiempo (primera vez que se subió). p3 &lt;- ggplot(df_grafo, aes(x=first_seen, y=positives)) + geom_line() + ylab(&#39;Positivos&#39;) + xlab(&#39;Tiempo&#39;) + theme_bw() ggplotly(p3) Regiones desde donde se subió: df_grafo %&gt;% select(times_submitted, Pais) %&gt;% group_by(Pais) %&gt;% summarise(times_submitted=sum(times_submitted)) %&gt;% ggplot(data=., aes(y=times_submitted)) + geom_bar(mapping = aes(fill=Pais, x=Pais), stat = &#39;identity&#39;) + theme_bw() Como los datos no están a escala, sumamos uno y aplicamos logaritmo para compararlos mejor: p_regiones &lt;- df_grafo %&gt;% select(times_submitted, Pais) %&gt;% group_by(Pais) %&gt;% summarise(times_submitted=sum(times_submitted)) %&gt;% ggplot(data=., aes(y=log(1+times_submitted))) + geom_bar(mapping = aes(fill=Pais, x=Pais), stat = &#39;identity&#39;) + ylab(&#39;Veces subido&#39;) + xlab(&#39;País&#39;) + theme_bw() ggplotly(p_regiones) Visualizamos el número de positivos por país: p_positivos_pais &lt;- df_grafo %&gt;% select(positives, Pais) %&gt;% group_by(Pais) %&gt;% summarise(positives=sum(positives)) %&gt;% ggplot(data=., aes(y=positives)) + geom_bar(mapping = aes(fill=Pais, x=Pais), stat = &#39;identity&#39;) + ylab(&#39;Positivos&#39;) + xlab(&#39;País&#39;) + theme_bw() ggplotly(p_positivos_pais) 3.7 Grafo de distancias &lt;= 10 para G Calculamos y dibujamos las componentes del grafo con distancias menores o iguales que diez. G &lt;- get_adj_matrix(10) c &lt;- components(G) biggest &lt;- which.max(c$csize) vids &lt;- V(G)[c$membership==biggest] plot(induced_subgraph(G, vids), edge.label=E(G)$dist) title(&quot;Distancias &lt;= 10 con pesos&quot;,cex.main=1,col.main=&quot;Black&quot;) plot(induced_subgraph(G, vids), vertex.size=25) title(&quot;Distancias &lt;= 10 sin pesos&quot;,cex.main=1,col.main=&quot;Black&quot;) plot(induced_subgraph(G, vids),vertex.size = 10, vertex.color = &quot;#1e3f66&quot;, vertex.frame.color = &#39;blue&#39;, vertex.label.cex = .7, vertex.label = NA, edge.curved = .5, edge.arrow.size = .3, edge.width = .7) title(&quot;Distancias &lt;= 10&quot;,cex.main=1,col.main=&quot;Black&quot;) 3.8 Análisis del mayor componente de G Todos los archivos tienen el mismo tamaño, subidos desde California en menos de una hora. Con una media de 20 positivos, probablemente sean el mismo archivo. v &lt;- as(vids, &#39;vector&#39;) compare &lt;- data.frame() for (i in v) { compare &lt;- rbind(compare, df[i,]) } compare %&gt;% select(size) %&gt;% unique() ## # A tibble: 1 × 1 ## size ## &lt;dbl&gt; ## 1 2669106 compare %&gt;% select(submission.submitter_country) %&gt;% unique() ## # A tibble: 1 × 1 ## submission.submitter_country ## &lt;chr&gt; ## 1 CA times &lt;- compare %&gt;% select(first_seen, scan_date) %&gt;% mutate(first_seen = gsub(&#39;20[0-9]{2}-[0-9]+-[0-9]+&#39;, &#39;&#39;, first_seen), scan_date = gsub(&#39;20[0-9]{2}-[0-9]+-[0-9]+&#39;, &#39;&#39;, scan_date)) Primera y última vez que se subió: lapply(times[,1], max) ## $first_seen ## [1] &quot; 00:46:00&quot; lapply(times[,1], min) ## $first_seen ## [1] &quot; 00:19:22&quot; Media: # Media compare %&gt;% select(positives) %&gt;% lapply(., mean) ## $positives ## [1] 19.85185 Desviación típica: # Desviación típica compare %&gt;% select(positives) %&gt;% lapply(., sd) ## $positives ## [1] 1.610153 3.9 Analizando los resultados de los antivirus de G Vamos a ver qué resultado da cada antivirus a los json (vértices) del grafo G. Si dos antivirus dan el mismo resultado en archivos diferentes que sabemos que son casi iguales es probable que compartan motor. La siguiente función recorre el directorio del dataset y crea un dataframe con los antivirus. # Entrada: Ruta al fichero # Salida: Dataframe con resultados de los AV get_results &lt;- function(json) { json_data &lt;- tidyjson::read_json(json) df_temp &lt;- json_data %&gt;% gather_object() %&gt;% filter(name==&#39;scans&#39;) %&gt;% spread_all() %&gt;% gather_object() %&gt;% select(ends_with(&#39;result&#39;)) %&gt;% .[1,] %&gt;% select(-last_col()) return(df_temp) } vids tiene los vértices de los grafos calculados. Pasamos los nombres a formato numérico con ceros a la izquierda. ficheros &lt;- sapply(vids, function(x) paste0(sprintf(&quot;%04d&quot;, x), &#39;.json&#39;) ) Creamos un dataframe y, por cada vértice, cogemos los escáneres. df_results &lt;- data.frame() for (i in ficheros) { df_results &lt;- rbind.fill(df_results, get_results(paste0(path,i))) } write.csv(df_results, &quot;~/Documentos/LCC/ProyectoVT/Proyecto/escaneres.csv&quot;) Leemos y limpiamos el dataframe. df_results &lt;- read_csv(&quot;~/Documentos/LCC/ProyectoVT/Proyecto/escaneres.csv&quot;) #df_results &lt;- df_results %&gt;% select(-..JSON) colnames(df_results) &lt;- lapply(colnames(df_results), function(x) gsub(&#39;.result&#39;, &#39;&#39;, x)) # Quitar columnas enteras NA df_results &lt;- df_results[, colSums(is.na(df_results)) != nrow(df_results)] df_results &lt;- df_results %&gt;% select(-...1) Cogemos la columna trece, que no tiene valores NA. Como en realidad todas las columnas son el mismo archivo con ver una sola nos sirve, y podemos ver cómo cada antivirus (excepto los que comparten motor) lo clasifican de manera distinta. j107 &lt;- df_results[13,] j107 &lt;- t(j107) knitr::kable(j107, col.names = c(&#39;0107.json&#39;)) 0107.json Lionic Trojan.AndroidOS.Wapnor.C!c ClamAV NA CAT-QuickHeal NA McAfee Artemis!46B876999B18 VIPRE NA Sangfor Malware.Generic-Script.Save.5b333b13 Alibaba NA K7GW Trojan ( 0051a3c71 ) Trustlook NA Arcabit NA Cyren AndroidOS/GhostPush.C.gen!Eldorado SymantecMobileInsight NA Symantec Trojan.Gen.MBT ESET-NOD32 a variant of Android/TrojanDropper.Shedun.V TrendMicro-HouseCall NA Avast Android:Revo-OU [Trj] Cynet Malicious (score: 99) Kaspersky HEUR:Trojan-Dropper.AndroidOS.Wapnor.a BitDefender NA NANO-Antivirus Trojan.Android.MLW.ebzlbe MicroWorld-eScan NA Rising NA Ad-Aware NA Emsisoft NA Comodo NA F-Secure Malware.ANDROID/Agent.hutg DrWeb Android.DownLoader.329.origin Zillya Dropper.Shedun.Android.200569 TrendMicro NA McAfee-GW-Edition Artemis!Trojan FireEye NA Sophos NA Ikarus Trojan-Dropper.AndroidOS.Shedun Avast-Mobile Android:Shedun-V [Trj] Jiangmin NA Avira ANDROID/Agent.hutg Antiy-AVL Trojan/Generic.ASBOL.A0C1 Kingsoft NA Microsoft TrojanDropper:AndroidOS/Shedun.A!MTB ZoneAlarm NA GData NA BitDefenderFalx NA AhnLab-V3 PUP/Android.Agent.839002 Tencent Dos.Trojan-dropper.Piom.Pege Yandex NA MAX malware (ai score=95) MaxSecure Android.wapnor.a Fortinet Android/Shedun.AC!tr AVG Android:Revo-OU [Trj] Si seleccionamos los AV Kasperky y ZoneAlarm se observa fácilmente que los resultados son idénticos y seguramente compartan el mismo motor. df_results %&gt;% select(Kaspersky, ZoneAlarm) %&gt;% head(., 10) %&gt;% knitr::kable(.) Kaspersky ZoneAlarm HEUR:Trojan-Dropper.AndroidOS.Hqwar.bk HEUR:Trojan-Dropper.AndroidOS.Hqwar.bk not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp NA not-a-virus:UDS:AdWare.AndroidOS.Ewind.kp not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp HEUR:Trojan-Dropper.AndroidOS.Wapnor.a NA not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp NA HEUR:Trojan-Banker.AndroidOS.Fakecalls.k HEUR:Trojan-Banker.AndroidOS.Fakecalls.k not-a-virus:HEUR:AdWare.AndroidOS.Adlo.b NA NA NA not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp not-a-virus:HEUR:AdWare.AndroidOS.Ewind.kp HEUR:Trojan.AndroidOS.Agent.aw HEUR:Trojan.AndroidOS.Agent.aw McAfee y McAfee GW Edition también, normal porque ambos son de McAfee df_results %&gt;% select(McAfee, `McAfee-GW-Edition`) %&gt;% head(., 10) %&gt;% knitr::kable(.) McAfee McAfee-GW-Edition Artemis!BF3E6F490724 NA Artemis!919A1900C529 Artemis Artemis!919A1900C529 Artemis Artemis!1E07E4821182 Artemis!Trojan Artemis!919A1900C529 Artemis NA NA Artemis!0C5C3D793761 Artemis!PUP Artemis!4EF2BDF37187 Artemis Artemis!919A1900C529 Artemis Artemis!19295D19D0AC Artemis!Trojan "],["regresion-reglas-de-asociación-y-clustering.html", "Chapter 4 Regresion, reglas de asociación y Clustering 4.1 Regresion 4.2 Reglas de asociación 4.3 Clustering", " Chapter 4 Regresion, reglas de asociación y Clustering 4.1 Regresion library(dplyr) library(tidyverse) library(ggplot2) library(readr) library(backports) virusTotal &lt;- read.csv(&#39;~/Documentos/LCC/ProyectoVT/Proyecto/virusTotal.csv&#39;) virusTotal &lt;- virusTotal %&gt;% mutate(Year = substr(virusTotal$first_seen, 0, 4)) virusTotal &lt;- virusTotal %&gt;% rename(country = submission.submitter_country) virusTotal &lt;- virusTotal %&gt;% rename(file_type = additional_info.exiftool.FileType) Pasamos el tamaño de los archivos a MB virusTotal &lt;- virusTotal %&gt;% mutate(size = virusTotal$size/1000000) Vamos a ver todas las gráficas de nuestro dataset: plot(virusTotal) Como vemos no parece que tengamos una relación directa por parte de dos variables, hemos pensado que estaría bien analizar o ver si hay una relacion entre el tamaño que tiene un archivo y si es positivo o no, ya que podriamos pensar de que al pesar más un archivo es más propenso a traer algun tipo de software malicioso. Vamos a analizar el tamaño frente al numero de positivos para ver si existe algun tipo de correlación ggplot2::ggplot(virusTotal,aes(x=size, y=positives))+geom_point()+geom_line() f1 &lt;- lm(positives~size, data = virusTotal) plot(f1) Vemos un resumen del modelo summary(f1) ## ## Call: ## lm(formula = positives ~ size, data = virusTotal) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.8717 -2.9135 -0.8898 3.0855 15.3370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.91526 0.34738 63.088 &lt; 2e-16 *** ## size -0.06463 0.01984 -3.258 0.00134 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.45 on 181 degrees of freedom ## Multiple R-squared: 0.0554, Adjusted R-squared: 0.05018 ## F-statistic: 10.62 on 1 and 181 DF, p-value: 0.001339 De aquí podemos sacar que la correlacion es prácticamente nula, solo el 5,54% de la variabilidad de los positivos tiene que ver con el tamaño, por lo tanto nuestro primer acercamiento es erróneo. No obstante, hemos dibujado la gráfica y observamos que la tendencia es que a menor número de positivos tanto mayor es tamaño del archivo analizado. Pero teniendo el cuenta los valores de R^2 ajustado y el p valor, este modelo no sirve para predecir y no podemos aceptar la hipótesis alternativa. ggplot(virusTotal, aes(x = size, y = positives)) + geom_point() + geom_line(aes(x = size, y = predict(f1,virusTotal)),col=&quot;blue&quot;) + geom_line() 4.2 Reglas de asociación library(arules) Utilizamos apriori para buscar todas las reglas posibles con suporte &gt; 0.28 y confianza &gt; 0.27. Hemos impuesto una longitud mínima de dos para filtrar las reglas con antecedente vacío. mis_reglas &lt;- apriori(virusTotal, parameter = list(supp = 0.28818, conf = 0.27634, minlen=2)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen maxlen target ## 0.27634 0.1 1 none FALSE TRUE 5 0.28818 2 10 rules ## ext ## TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 52 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[408 item(s), 183 transaction(s)] done [0.00s]. ## sorting and recoding items ... [21 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 done [0.00s]. ## writing ... [75 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. length(mis_reglas) ## [1] 75 Este es el número de reglas que podemos encontrar en nuestro dataset. El resumen de las reglas obtenidas sería el siguiente: summary(mis_reglas) ## set of 75 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 ## 40 27 8 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.000 2.000 2.573 3.000 4.000 ## ## summary of quality measures: ## support confidence coverage lift count ## Min. :0.2896 Min. :0.4240 Min. :0.2896 Min. :1.158 Min. : 53.00 ## 1st Qu.:0.2951 1st Qu.:0.5773 1st Qu.:0.3060 1st Qu.:1.406 1st Qu.: 54.00 ## Median :0.3060 Median :0.9474 Median :0.3333 Median :1.500 Median : 56.00 ## Mean :0.3392 Mean :0.8311 Mean :0.4377 Mean :1.751 Mean : 62.08 ## 3rd Qu.:0.3333 3rd Qu.:1.0000 3rd Qu.:0.5519 3rd Qu.:1.812 3rd Qu.: 61.00 ## Max. :0.6339 Max. :1.0000 Max. :0.6831 Max. :3.155 Max. :116.00 ## ## mining info: ## data ntransactions support confidence ## virusTotal 183 0.28818 0.27634 ## call ## apriori(data = virusTotal, parameter = list(supp = 0.28818, conf = 0.27634, minlen = 2)) Como podemos ver en el resumen, nos muestran datos de las reglas generadas, podemos ver que la media obtenida en el soporte es de 0.28818, lo cual no es muy alto e indica que la media de las reglas obtenidas no son muy frecuentes. La confianza tiene una media de 0.27634, es decir, 27.63% de las X también contienen a Y. Estas son todas las reglas calculadas inspect(mis_reglas) ## lhs rhs support confidence coverage lift count ## [1] {country=CA} =&gt; {times_submitted=[1,2)} 0.3060109 1.0000000 0.3060109 1.811881 56 ## [2] {times_submitted=[1,2)} =&gt; {country=CA} 0.3060109 0.5544554 0.5519126 1.811881 56 ## [3] {country=CA} =&gt; {Year=2021} 0.3060109 1.0000000 0.3060109 1.500000 56 ## [4] {Year=2021} =&gt; {country=CA} 0.3060109 0.4590164 0.6666667 1.500000 56 ## [5] {country=CA} =&gt; {file_type=ZIP} 0.3060109 1.0000000 0.3060109 1.464000 56 ## [6] {file_type=ZIP} =&gt; {country=CA} 0.3060109 0.4480000 0.6830601 1.464000 56 ## [7] {file_type=DEX} =&gt; {country=CZ} 0.3169399 1.0000000 0.3169399 3.155172 58 ## [8] {country=CZ} =&gt; {file_type=DEX} 0.3169399 1.0000000 0.3169399 3.155172 58 ## [9] {file_type=DEX} =&gt; {times_submitted=[2,3.54e+03]} 0.2950820 0.9310345 0.3169399 2.077796 54 ## [10] {times_submitted=[2,3.54e+03]} =&gt; {file_type=DEX} 0.2950820 0.6585366 0.4480874 2.077796 54 ## [11] {country=CZ} =&gt; {times_submitted=[2,3.54e+03]} 0.2950820 0.9310345 0.3169399 2.077796 54 ## [12] {times_submitted=[2,3.54e+03]} =&gt; {country=CZ} 0.2950820 0.6585366 0.4480874 2.077796 54 ## [13] {...1=[122,183]} =&gt; {X=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [14] {X=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [15] {size=[3.03,178]} =&gt; {Year=2021} 0.2896175 0.8688525 0.3333333 1.303279 53 ## [16] {Year=2021} =&gt; {size=[3.03,178]} 0.2896175 0.4344262 0.6666667 1.303279 53 ## [17] {size=[3.03,178]} =&gt; {file_type=ZIP} 0.3005464 0.9016393 0.3333333 1.320000 55 ## [18] {file_type=ZIP} =&gt; {size=[3.03,178]} 0.3005464 0.4400000 0.6830601 1.320000 55 ## [19] {X=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [20] {...1=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [21] {X=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [22] {...1=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 1.0000000 0.3333333 3.000000 61 ## [23] {size=[1.85,3.03)} =&gt; {times_submitted=[1,2)} 0.2950820 0.8852459 0.3333333 1.603960 54 ## [24] {times_submitted=[1,2)} =&gt; {size=[1.85,3.03)} 0.2950820 0.5346535 0.5519126 1.603960 54 ## [25] {size=[1.85,3.03)} =&gt; {Year=2021} 0.3114754 0.9344262 0.3333333 1.401639 57 ## [26] {Year=2021} =&gt; {size=[1.85,3.03)} 0.3114754 0.4672131 0.6666667 1.401639 57 ## [27] {size=[1.85,3.03)} =&gt; {file_type=ZIP} 0.3005464 0.9016393 0.3333333 1.320000 55 ## [28] {file_type=ZIP} =&gt; {size=[1.85,3.03)} 0.3005464 0.4400000 0.6830601 1.320000 55 ## [29] {positives=[19,23)} =&gt; {file_type=ZIP} 0.2896175 0.7910448 0.3661202 1.158090 53 ## [30] {file_type=ZIP} =&gt; {positives=[19,23)} 0.2896175 0.4240000 0.6830601 1.158090 53 ## [31] {total=[62,64]} =&gt; {Year=2021} 0.3497268 0.8888889 0.3934426 1.333333 64 ## [32] {Year=2021} =&gt; {total=[62,64]} 0.3497268 0.5245902 0.6666667 1.333333 64 ## [33] {total=[62,64]} =&gt; {file_type=ZIP} 0.3934426 1.0000000 0.3934426 1.464000 72 ## [34] {file_type=ZIP} =&gt; {total=[62,64]} 0.3934426 0.5760000 0.6830601 1.464000 72 ## [35] {times_submitted=[1,2)} =&gt; {Year=2021} 0.5519126 1.0000000 0.5519126 1.500000 101 ## [36] {Year=2021} =&gt; {times_submitted=[1,2)} 0.5519126 0.8278689 0.6666667 1.500000 101 ## [37] {times_submitted=[1,2)} =&gt; {file_type=ZIP} 0.5300546 0.9603960 0.5519126 1.406020 97 ## [38] {file_type=ZIP} =&gt; {times_submitted=[1,2)} 0.5300546 0.7760000 0.6830601 1.406020 97 ## [39] {Year=2021} =&gt; {file_type=ZIP} 0.6338798 0.9508197 0.6666667 1.392000 116 ## [40] {file_type=ZIP} =&gt; {Year=2021} 0.6338798 0.9280000 0.6830601 1.392000 116 ## [41] {times_submitted=[1,2), ## country=CA} =&gt; {Year=2021} 0.3060109 1.0000000 0.3060109 1.500000 56 ## [42] {country=CA, ## Year=2021} =&gt; {times_submitted=[1,2)} 0.3060109 1.0000000 0.3060109 1.811881 56 ## [43] {times_submitted=[1,2), ## Year=2021} =&gt; {country=CA} 0.3060109 0.5544554 0.5519126 1.811881 56 ## [44] {times_submitted=[1,2), ## country=CA} =&gt; {file_type=ZIP} 0.3060109 1.0000000 0.3060109 1.464000 56 ## [45] {country=CA, ## file_type=ZIP} =&gt; {times_submitted=[1,2)} 0.3060109 1.0000000 0.3060109 1.811881 56 ## [46] {times_submitted=[1,2), ## file_type=ZIP} =&gt; {country=CA} 0.3060109 0.5773196 0.5300546 1.886598 56 ## [47] {country=CA, ## Year=2021} =&gt; {file_type=ZIP} 0.3060109 1.0000000 0.3060109 1.464000 56 ## [48] {country=CA, ## file_type=ZIP} =&gt; {Year=2021} 0.3060109 1.0000000 0.3060109 1.500000 56 ## [49] {file_type=ZIP, ## Year=2021} =&gt; {country=CA} 0.3060109 0.4827586 0.6338798 1.577586 56 ## [50] {country=CZ, ## file_type=DEX} =&gt; {times_submitted=[2,3.54e+03]} 0.2950820 0.9310345 0.3169399 2.077796 54 ## [51] {times_submitted=[2,3.54e+03], ## file_type=DEX} =&gt; {country=CZ} 0.2950820 1.0000000 0.2950820 3.155172 54 ## [52] {times_submitted=[2,3.54e+03], ## country=CZ} =&gt; {file_type=DEX} 0.2950820 1.0000000 0.2950820 3.155172 54 ## [53] {size=[1.85,3.03), ## times_submitted=[1,2)} =&gt; {Year=2021} 0.2950820 1.0000000 0.2950820 1.500000 54 ## [54] {size=[1.85,3.03), ## Year=2021} =&gt; {times_submitted=[1,2)} 0.2950820 0.9473684 0.3114754 1.716519 54 ## [55] {times_submitted=[1,2), ## Year=2021} =&gt; {size=[1.85,3.03)} 0.2950820 0.5346535 0.5519126 1.603960 54 ## [56] {size=[1.85,3.03), ## times_submitted=[1,2)} =&gt; {file_type=ZIP} 0.2896175 0.9814815 0.2950820 1.436889 53 ## [57] {size=[1.85,3.03), ## file_type=ZIP} =&gt; {times_submitted=[1,2)} 0.2896175 0.9636364 0.3005464 1.745995 53 ## [58] {times_submitted=[1,2), ## file_type=ZIP} =&gt; {size=[1.85,3.03)} 0.2896175 0.5463918 0.5300546 1.639175 53 ## [59] {size=[1.85,3.03), ## Year=2021} =&gt; {file_type=ZIP} 0.2950820 0.9473684 0.3114754 1.386947 54 ## [60] {size=[1.85,3.03), ## file_type=ZIP} =&gt; {Year=2021} 0.2950820 0.9818182 0.3005464 1.472727 54 ## [61] {file_type=ZIP, ## Year=2021} =&gt; {size=[1.85,3.03)} 0.2950820 0.4655172 0.6338798 1.396552 54 ## [62] {total=[62,64], ## Year=2021} =&gt; {file_type=ZIP} 0.3497268 1.0000000 0.3497268 1.464000 64 ## [63] {total=[62,64], ## file_type=ZIP} =&gt; {Year=2021} 0.3497268 0.8888889 0.3934426 1.333333 64 ## [64] {file_type=ZIP, ## Year=2021} =&gt; {total=[62,64]} 0.3497268 0.5517241 0.6338798 1.402299 64 ## [65] {times_submitted=[1,2), ## Year=2021} =&gt; {file_type=ZIP} 0.5300546 0.9603960 0.5519126 1.406020 97 ## [66] {times_submitted=[1,2), ## file_type=ZIP} =&gt; {Year=2021} 0.5300546 1.0000000 0.5300546 1.500000 97 ## [67] {file_type=ZIP, ## Year=2021} =&gt; {times_submitted=[1,2)} 0.5300546 0.8362069 0.6338798 1.515108 97 ## [68] {times_submitted=[1,2), ## country=CA, ## Year=2021} =&gt; {file_type=ZIP} 0.3060109 1.0000000 0.3060109 1.464000 56 ## [69] {times_submitted=[1,2), ## country=CA, ## file_type=ZIP} =&gt; {Year=2021} 0.3060109 1.0000000 0.3060109 1.500000 56 ## [70] {country=CA, ## file_type=ZIP, ## Year=2021} =&gt; {times_submitted=[1,2)} 0.3060109 1.0000000 0.3060109 1.811881 56 ## [71] {times_submitted=[1,2), ## file_type=ZIP, ## Year=2021} =&gt; {country=CA} 0.3060109 0.5773196 0.5300546 1.886598 56 ## [72] {size=[1.85,3.03), ## times_submitted=[1,2), ## Year=2021} =&gt; {file_type=ZIP} 0.2896175 0.9814815 0.2950820 1.436889 53 ## [73] {size=[1.85,3.03), ## times_submitted=[1,2), ## file_type=ZIP} =&gt; {Year=2021} 0.2896175 1.0000000 0.2896175 1.500000 53 ## [74] {size=[1.85,3.03), ## file_type=ZIP, ## Year=2021} =&gt; {times_submitted=[1,2)} 0.2896175 0.9814815 0.2950820 1.778328 53 ## [75] {times_submitted=[1,2), ## file_type=ZIP, ## Year=2021} =&gt; {size=[1.85,3.03)} 0.2896175 0.5463918 0.5300546 1.639175 53 Vamos a ordenar las reglas por lift y mostramos las diez reglas con más lift mis_reglas_lift &lt;- sort(mis_reglas, by = &quot;lift&quot;) inspect(mis_reglas_lift[1:10]) ## lhs rhs support ## [1] {file_type=DEX} =&gt; {country=CZ} 0.3169399 ## [2] {country=CZ} =&gt; {file_type=DEX} 0.3169399 ## [3] {times_submitted=[2,3.54e+03], file_type=DEX} =&gt; {country=CZ} 0.2950820 ## [4] {times_submitted=[2,3.54e+03], country=CZ} =&gt; {file_type=DEX} 0.2950820 ## [5] {...1=[122,183]} =&gt; {X=[122,183]} 0.3333333 ## [6] {X=[122,183]} =&gt; {...1=[122,183]} 0.3333333 ## [7] {X=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 ## [8] {...1=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 ## [9] {X=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 ## [10] {...1=[1,61.7)} =&gt; {X=[1,61.7)} 0.3333333 ## confidence coverage lift count ## [1] 1 0.3169399 3.155172 58 ## [2] 1 0.3169399 3.155172 58 ## [3] 1 0.2950820 3.155172 54 ## [4] 1 0.2950820 3.155172 54 ## [5] 1 0.3333333 3.000000 61 ## [6] 1 0.3333333 3.000000 61 ## [7] 1 0.3333333 3.000000 61 ## [8] 1 0.3333333 3.000000 61 ## [9] 1 0.3333333 3.000000 61 ## [10] 1 0.3333333 3.000000 61 Vamos a ordenar las reglas por support y mostramos las diez reglas con más soporte mis_reglas_support &lt;- sort(mis_reglas, by = &quot;support&quot;) inspect(mis_reglas_support[1:10]) ## lhs rhs support ## [1] {Year=2021} =&gt; {file_type=ZIP} 0.6338798 ## [2] {file_type=ZIP} =&gt; {Year=2021} 0.6338798 ## [3] {times_submitted=[1,2)} =&gt; {Year=2021} 0.5519126 ## [4] {Year=2021} =&gt; {times_submitted=[1,2)} 0.5519126 ## [5] {times_submitted=[1,2)} =&gt; {file_type=ZIP} 0.5300546 ## [6] {file_type=ZIP} =&gt; {times_submitted=[1,2)} 0.5300546 ## [7] {times_submitted=[1,2), Year=2021} =&gt; {file_type=ZIP} 0.5300546 ## [8] {times_submitted=[1,2), file_type=ZIP} =&gt; {Year=2021} 0.5300546 ## [9] {file_type=ZIP, Year=2021} =&gt; {times_submitted=[1,2)} 0.5300546 ## [10] {total=[62,64]} =&gt; {file_type=ZIP} 0.3934426 ## confidence coverage lift count ## [1] 0.9508197 0.6666667 1.392000 116 ## [2] 0.9280000 0.6830601 1.392000 116 ## [3] 1.0000000 0.5519126 1.500000 101 ## [4] 0.8278689 0.6666667 1.500000 101 ## [5] 0.9603960 0.5519126 1.406020 97 ## [6] 0.7760000 0.6830601 1.406020 97 ## [7] 0.9603960 0.5519126 1.406020 97 ## [8] 1.0000000 0.5300546 1.500000 97 ## [9] 0.8362069 0.6338798 1.515108 97 ## [10] 1.0000000 0.3934426 1.464000 72 Como sabemos, el soporte nos indica en qué porcentaje el patrón encontrado aparece en el dataset, y vemos que cuando el año es 2021 en el 63,3% de la veces el tipo de archivo es ZIP. Vamos a ordenar las reglas por confianza y mostramos las diez reglas con más confianza mis_reglas_confidence &lt;- sort(mis_reglas, by = &quot;confidence&quot;) inspect(mis_reglas_confidence[1:10]) ## lhs rhs support confidence coverage lift ## [1] {country=CA} =&gt; {times_submitted=[1,2)} 0.3060109 1 0.3060109 1.811881 ## [2] {country=CA} =&gt; {Year=2021} 0.3060109 1 0.3060109 1.500000 ## [3] {country=CA} =&gt; {file_type=ZIP} 0.3060109 1 0.3060109 1.464000 ## [4] {file_type=DEX} =&gt; {country=CZ} 0.3169399 1 0.3169399 3.155172 ## [5] {country=CZ} =&gt; {file_type=DEX} 0.3169399 1 0.3169399 3.155172 ## [6] {...1=[122,183]} =&gt; {X=[122,183]} 0.3333333 1 0.3333333 3.000000 ## [7] {X=[122,183]} =&gt; {...1=[122,183]} 0.3333333 1 0.3333333 3.000000 ## [8] {X=[61.7,122)} =&gt; {...1=[61.7,122)} 0.3333333 1 0.3333333 3.000000 ## [9] {...1=[61.7,122)} =&gt; {X=[61.7,122)} 0.3333333 1 0.3333333 3.000000 ## [10] {X=[1,61.7)} =&gt; {...1=[1,61.7)} 0.3333333 1 0.3333333 3.000000 ## count ## [1] 56 ## [2] 56 ## [3] 56 ## [4] 58 ## [5] 58 ## [6] 61 ## [7] 61 ## [8] 61 ## [9] 61 ## [10] 61 De aquí podemos sacar que el pais CA solo ha enviado archivos de tipo ZIP en 2021, o también como mostramos en la parte de visualización que República Checa siempre envia DEX como tipo de archivo. 4.3 Clustering library(magrittr) Preparamos el kmeans con 4 centros y usaremos el tamaño frente al número de positivos set.seed(1) virus &lt;- virusTotal %&gt;% select(c(&quot;size&quot;,&quot;positives&quot;))%&gt;% kmeans(centers=4, nstart=10) str(virus) ## List of 9 ## $ cluster : int [1:183] 4 4 4 2 4 4 4 4 4 2 ... ## $ centers : num [1:4, 1:2] 178.36 1.76 74.81 4.49 15 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. ..$ : chr [1:2] &quot;size&quot; &quot;positives&quot; ## $ totss : num 54130 ## $ withinss : num [1:4] 0 912 1675 3606 ## $ tot.withinss: num 6193 ## $ betweenss : num 47937 ## $ size : int [1:4] 1 65 3 114 ## $ iter : int 2 ## $ ifault : int 0 ## - attr(*, &quot;class&quot;)= chr &quot;kmeans&quot; Vemos el tamaño de los clusters y de los centroides virus$size ## [1] 1 65 3 114 virus$centers ## size positives ## 1 178.355426 15.00000 ## 2 1.761319 26.61538 ## 3 74.811670 16.00000 ## 4 4.487114 18.86842 Ahora vamos a visualizar un gráfico del tamaño (eje X) frente al número de positivos (eje Y), lo dibujaremos con colores según el cluster al que pertenezca y la etiqueta será el tipo de archivo analizado. Como la gráfica quedaba muy comprimida la vamos a mostrar por partes para un mejor entendimiento plot(virusTotal$size,virusTotal$positives, type=&quot;n&quot;,xlim = c(0,10), xlab = &quot;size&quot;,ylab = &quot;positives&quot;) text(x=virusTotal$size, y=virusTotal$positives, labels=virusTotal$file_type, col=virus$cluster+1) Aqui vemos los archivos de entre 0 y 10 MB que es donde se concentran la mayoría plot(virusTotal$size,virusTotal$positives, type=&quot;n&quot;,xlim = c(11,100), xlab = &quot;size&quot;,ylab = &quot;positives&quot;) text(x=virusTotal$size, y=virusTotal$positives, labels=virusTotal$file_type, col=virus$cluster+1) Aqui los archivos entre 11 y 100 MB que aunque haya menos también podemos ver los clusters en los que se agrupan plot(virusTotal$size,virusTotal$positives, type=&quot;n&quot;, xlab = &quot;size&quot;,ylab = &quot;positives&quot;) text(x=virusTotal$size, y=virusTotal$positives, labels=virusTotal$file_type, col=virus$cluster+1) Y por ultimo aqui ya vemos la gráfica al completo "],["conclusiones.html", "Chapter 5 Conclusiones", " Chapter 5 Conclusiones Del análisis exploratorio podemos extraer mucha información general del dataset. Los archivos tienen un tamaño medio de unos 6MB y vienen en formato ZIP, entre quince y veinticinco antivirus los detectan como positivos de media y la gran mayoría se han subido en 2021. El análisis del ssdeep ha mostrado en primer lugar que hay catorce archivos cuya distancia es uno o cero. Indicando por lo tanto que se trata prácticamente del mismo archivo, esto puede deberse a que alguien se haya dedicado a subir muchas veces el mismo archivo con pequeñas modificaciones. Fueron subidos todos desde CA en poco tiempo, el tamaño solo difiere en un archivo y todos fueron vistos por primera la vez que fueron escaneados, no fueron subidos nunca antes. Sin embargo, los antivirus han dado distintos resultado cada vez (manteniendo entre 19 y 22 positivos). Del grafo G1 se ha obtenido un componente de 99 que comparten al menos un 60% de código, lo que es más del dataset. Al analizar este componente hay varios nodos que tienen un grado muy alto, lo que quiere decir que comparten código con muchos otros nodos. Este caso es parecido al grafo de antes, se subieron los 99 archivos en un intervalo de nueve horas (unos once por hora) casi todos desde Bolivia y algunas desde República Checa y Canadá. A pesar de que Bolivia subió la gran parte, República Checa es el que suma mayor tasa de positivos. Al calcular los componentes de G1, al ser un no dirigido lo único que hacemos es buscar vértices desde donde se puede llegar a cualquier otro vértice por algún camino. Se han encontrado dos grandes componentes de tamaños 12 y 99. Dentro del componente de 99 a su vez se observa que se forman subgrupos, como por la forma de construir el grafo sabemos que aquellos que están conectados sabemos tienen similitudes en su código, estos subgrupos pueden ser pequeñas variantes del mismo malware. Del grafo G se obtienen conclusiones parecidas, son 27 archivos muy parecidos subidos en menos de una hora con unos veinte positivos de media. Igual que antes, lo más seguro es que se trate de la misma persona subiendo 27 veces lo mismo con pequeñas modificaciones. Para finalizar este capítulo se aprovechan los grafos para ver cómo han clasificado los antivirus esos archivos que sabemos que son casi idénticos. Si hay varios que los clasifican igual o muy parecido, podemos sospechar que comparten motor. No parece que los antivirus sigan algún patrón para clasificar, de hecho en algunos parecen cadenas de caracteres aleatorias. De ahí que analizar estos resultados es bastante complicado. La mayoría de los AV parace estar de acuerdo en que los archivos del grafo G son Adware, otros dicen que son troyanos. Lo interesante es que se ha podido ver con claridad que tanto Kaspersky como ZoneAlarm lo clasifican casi igual, lo mismo pasa con McAfee y McAfee-GW-Edition. Por tanto es muy probable que los dos primeros compartan motor, y es seguro que los dos últimos lo hagan (son de la misma empresa). "],["referencias.html", "Chapter 6 Referencias", " Chapter 6 Referencias https://cran.microsoft.com/snapshot/2017-08-01/web/packages/tidyjson/vignettes/introduction-to-tidyjson.html https://bookdown.org/ https://igraph.org/r/html/latest/ https://ssdeep-project.github.io/ssdeep/index.html https://nikhilh20.medium.com/fuzzy-hashing-ssdeep-3cade6931b72 "]]
